{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# __Image Classification with Convolutional Neural Networks (CNN's)__\n",
    "\n",
    "#### Author: Alexander Fred Ojala\n",
    "\n",
    "**Sources:** Francois Chollet, Tensorflow docs, LeNet (https://engmrk.com/lenet-5-a-classic-cnn-architecture/)\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap DNN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST info\n",
    "The MNIST data is split into three parts: 60,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test).\n",
    "\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST-Matrix.png](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a very rich structure (warning: computationally intensive visualizations).\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [60000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![https://www.tensorflow.org/images/mnist-train-xs.png](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n",
    "                            datasets.mnist.load_data()\n",
    "\n",
    "# input information\n",
    "print('Train input shape:',x_train.shape)\n",
    "print('Test input shape:',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[2],cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 10 outputs:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for DNN\n",
    "\n",
    "# Normalize the data, flatten inputs, and convert datatype\n",
    "x_train = x_train.reshape(60000, 28*28). \\\n",
    "                    astype('float32') / 255 #784\n",
    "\n",
    "x_test = x_test.reshape(10000, 28*28) \\\n",
    "                    .astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model constructor\n",
    "model = Sequential()\n",
    "# Add layers sequentially\n",
    "model.add(Dense(300, activation='relu', \\\n",
    "                    input_shape=(784,)))\n",
    "\n",
    "# Second..\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "# Third..\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "NO_EPOCHS = 10\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=NO_EPOCHS,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test,verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(hist):\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([None] + hist.history['accuracy'], 'o-')\n",
    "    ax.plot([None] + hist.history['val_accuracy'], 'x-')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "    ax.set_title('Training/Validation acc per Epoch')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Acc') \n",
    "    plt.plot()\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([None] + hist.history['loss'], 'o-',c='r')\n",
    "    ax.plot([None] + hist.history['val_loss'], 'x-',c='g')\n",
    "    # Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Train loss', 'Validation loss'], loc = 0)\n",
    "    ax.set_title('Training/Validation loss per Epoch')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Acc') \n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Training loss is lower for the first epochs, because:**\n",
    "\n",
    "*The training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification w/ CNNs\n",
    "Import MNIST data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import pandas\n",
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = pandas.get_dummies(y_train).values\n",
    "y_test = pandas.get_dummies(y_test).values\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1) # add depth dimension of the inputs\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla / Baseline CNN \n",
    "## LeNet, used for digit classification in the 90's\n",
    "Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner proposed a neural network architecture for handwritten character recognition in 1990’s which they called LeNet-5. This is oftentimes used as the baseline CNN model that is trained.\n",
    "\n",
    "![](./imgs/lenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First layer:\n",
    "\n",
    "Input is a 32×32 grayscale image which passes through the first convolutional layer with 6 feature maps or filters having size 5×5 and a stride of one. The image dimensions changes from 32x32x1 to 28x28x6.\n",
    "\n",
    "![](imgs/lenet1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), \\\n",
    "                        activation='relu', input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer:\n",
    "\n",
    "Then the LeNet-5 applies average pooling layer or sub-sampling layer with a filter size 2×2 and a stride of two. The resulting image dimensions will be reduced to 14x14x6.\n",
    "\n",
    "![](imgs/lenet2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second layer\n",
    "model.add(layers.AveragePooling2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer:\n",
    "\n",
    "Next, there is a second convolutional layer with 16 feature maps having size 5×5 and a stride of 1. The main reason is to break the symmetry in the network and keeps the number of connections within reasonable bounds.\n",
    "\n",
    "![](imgs/lenet3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third layer\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Layer:\n",
    "\n",
    "The fourth layer (S4) is again an average pooling layer with filter size 2×2 and a stride of 2.\n",
    "\n",
    "![](imgs/lenet4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth layer\n",
    "model.add(layers.AveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Layer:\n",
    "\n",
    "The fifth layer (C5) is a fully connected convolutional layer with 120 feature maps each of size 1×1. Each of the 120 units in C5 is connected to all the 400 nodes (5x5x16) in the fourth layer S4. \n",
    "\n",
    "![](imgs/lenet5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=120, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixth Layer:\n",
    "\n",
    "The sixth layer is a fully connected layer (F6) with 84 units.\n",
    "\n",
    "![](imgs/lenet6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sixth layer\n",
    "model.add(layers.Dense(units=84, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer:\n",
    "\n",
    "Finally, there is a fully connected softmax output layer ŷ with 10 possible values corresponding to the digits from 0 to 9.\n",
    "Fully Connected Output Layer\n",
    "\n",
    "![](imgs/lenet7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(layers.Dense(units=10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary:\n",
    "\n",
    "![](imgs/lenet_summary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \\\n",
    "              optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x=x_train,y=y_train, epochs=10, \\\n",
    "                 batch_size=128, validation_data=(x_test, y_test), \\\n",
    "                 verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model accuracy / epoch\n",
    "plot_loss_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazing improvement!\n",
    "(1-hist.history['val_accuracy'][-1])/(1-history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "Great accuracy in just a couple of epochs. No regularization applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "> #### [Part 1: Keras Tensorflow + all dependencies](#Part-1:-Install-Keras-+-Tensorflow)\n",
    "\n",
    "> #### [Part 2: Training from scratch + data augmentation](#Part-2:-Extract-bottleneck-features-from-the-data-set)\n",
    "\n",
    "> #### [Part 3: Transfer Learning. Extract bottleneck features from the data set](#Part-2:-Extract-bottleneck-features-from-the-data-set)\n",
    "\n",
    "> #### [Part 4: Train the top layer of your CNN](#Part-3:-Train-the-top-layer-of-your-CNN)\n",
    "\n",
    "> #### [Part 5: Make predicitons on the mixed test images](#Part-4:-Validate-accuracy-and-make-predictions-on-unlabeled-data)\n",
    "\n",
    "### Old material\n",
    "\n",
    "> #### [TRAIN NETWORK TO CLASSIFY 50 IMAGE CLASSES w Theano](#Part-5----right-now-built-for-Theano)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec1'></div>\n",
    "\n",
    "# Part 1: Install Keras + Tensorflow\n",
    "\n",
    "- **Install TensorFlow:** Run the command `conda install tensorflow`\n",
    "- **Install Keras:** Run the command `conda install keras`\n",
    "\n",
    "See https://keras.io/#installation and https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that TensorFlow is used as the backend\n",
    "\n",
    "**Read more at:** https://keras.io/backend/#switching-from-one-backend-to-another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_image_dim_ordering('tf') # note that we need to have tensorflow dimension ordering still because of the weigths.\n",
    "#print('The backend is:',K.backend())\n",
    "#import tensorflow as tf\n",
    "#print(K.image_dim_ordering()) # should say tf\n",
    "#print(tf.__version__) # tested for 1.11.0\n",
    "#\n",
    "#import keras\n",
    "#print(keras.__version__) # tested for 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from __future__ import absolute_import, division, print_function # make it compatible w Python 2\n",
    "import os\n",
    "import h5py # to handle weights\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 #conda install open-cv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at files, note all cat images and dog images are unique\n",
    "for path, dirs, files in os.walk('./data'):\n",
    "    print('FOLDER',path)\n",
    "    for f in files[:2]:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of cat training images:', len(next(os.walk('./data/train/cats'))[2]))\n",
    "print('Number of dog training images:', len(next(os.walk('./data/train/dogs'))[2]))\n",
    "print('Number of cat validation images:', len(next(os.walk('./data/validation/cats'))[2]))\n",
    "print('Number of dog validation images:', len(next(os.walk('./data/validation/dogs'))[2]))\n",
    "print('Number of uncategorized test images:', len(next(os.walk('./data/test/catvdog'))[2]))\n",
    "\n",
    "# There should be 1000 train cat images, 1000 train dogs, \n",
    "# 400 validation cats, 400 validation dogs, 100 uncategorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "TRAIN_DIR = './data/train/'\n",
    "VAL_DIR = './data/validation/'\n",
    "TEST_DIR = './data/test/' #mixed cats and dogs\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "n_train_samples = 2000\n",
    "n_validation_samples = 800\n",
    "n_epoch = 20\n",
    "n_test_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "# Part 2: Training a small convnet from scratch\n",
    "\n",
    "The right tool for an image classification job is a convnet, so let's try to train one on our data, as an initial baseline. Very similar to LeNet, as a baseline. Roughly ~75% accuracy. Takes 30mins to train on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preprocessing\n",
    "\n",
    "Data should be formatted into appropriately pre-processed floating point tensors before being fed into our network. Currently, our data is JPEG files, so the steps for getting it into our network are roughly:\n",
    "\n",
    "    Read the picture files.\n",
    "    Decode the JPEG content to RBG grids of pixels.\n",
    "    Convert these into floating point tensors.\n",
    "    Rescale the pixel values (between 0 and 255) to the [0, 1] interval.\n",
    "\n",
    "Keras has utilities to take care of these steps automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        TRAIN_DIR,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20,)). 20 is the number of samples in each batch (the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's fit our model to the data using the generator. We do it using the `fit_generator method`, the equivalent of fit for data generators like ours. Need to define steps_per_epoch because of generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_scratch = history\n",
    "\n",
    "#model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mod = load_model('cats_and_dogs_small_1.h5')\n",
    "mod.summary()  # As a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.evaluate_generator(validation_generator,steps=40,verbose=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = history_scratch\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "![](imgs/cnn_scratch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "These plots are characteristic of overfitting. Our training accuracy increases until it reaches nearly 100%, while our validation accuracy stalls at 70-72%.\n",
    "\n",
    "Because we only have relatively few training samples (2000), overfitting is going to be our number one concern. \n",
    "\n",
    "Overfitting can be mitigated using dropout and weight decay (L2 regularization). We are now going to introduce a new one, specific to computer vision, and used almost universally when processing images with deep learning models: data augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data augmentation\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. \n",
    "\n",
    "Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since we only have few examples, our number one concern should be overfitting. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image data generator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40, #rotation_range degrees (0-180), range that randomly rotate pictures\n",
    "        width_shift_range=0.2, #width_shift range (fraction of total width) within which to randomly translate pic\n",
    "        height_shift_range=0.2, # -ii-\n",
    "        \n",
    "        #rescale value we multiply the data before any other processing. \n",
    "        #Our original images consist in RGB coefficients in the 0-255, \n",
    "        #but such values would be too high for our models to process (given typical learning rate), \n",
    "        # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "        rescale=1./255,\n",
    "        \n",
    "        #randomly applying shearing transformations (shear mapping is a linear map that \n",
    "        #displaces each point in fixed direction, by an amount proportional to its \n",
    "        #signed distance from a line that is parallel to that direction)\n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2, #randomly zooming inside pictures\n",
    "        \n",
    "        #is for randomly flipping half of the images horizontally \n",
    "        #--relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n",
    "\n",
    "        horizontal_flip=True,\n",
    "    \n",
    "        #is the strategy used for filling in newly created pixels, \n",
    "        #which can appear after a rotation or a width/height shift.\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing --we disable rescaling in this case to keep the images displayable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = load_img(TRAIN_DIR+'cats/cat0002.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if not os.path.exists('preview'):\n",
    "    os.makedirs('preview')\n",
    "\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "prev_files = next(os.walk('./preview'))[2]\n",
    "print(prev_files[:4])\n",
    "\n",
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    im = np.array(Image.open(file_path))\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return im\n",
    "\n",
    "def plot_pic(img):\n",
    "    # Plot openCV pic\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for img in prev_files[:10]:\n",
    "    print('Image '+img)\n",
    "    plot_pic('./preview/'+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model construction\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "'''\n",
    "On top of it we stick two fully-connected layers. \n",
    "We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. \n",
    "To go with it we will also use the binary_crossentropy loss to train our model.\n",
    "'''\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        TRAIN_DIR,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=40,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mod = load_model('cats_and_dogs_small_2.h5')\n",
    "mod.summary()  # As a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.evaluate_generator(validation_generator,steps=40,verbose=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![](imgs/new_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot picture and print class prediction on cats vs dogs (unsorted)\n",
    "\n",
    "\n",
    "try_images =  [TEST_DIR+'catvdog/'+img for img in os.listdir(TEST_DIR+'catvdog/')]\n",
    "\n",
    "def predict(mod,i=0,r=None):\n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        if 'DS_Store' in try_images[idx]:\n",
    "            continue\n",
    "        \n",
    "        img_path = try_images[idx]\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        class_pred = mod.predict_classes(x,verbose=0)\n",
    "        \n",
    "        if class_pred == 0:\n",
    "            class_guess='CAT'\n",
    "        else:\n",
    "            class_guess='DOG'\n",
    "        \n",
    "        print('\\n\\nI think this is a ' + class_guess)\n",
    "        plot_pic(try_images[idx])\n",
    "\n",
    "predict(mod,r=range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Transfer Learning\n",
    "## Build a more powerful your own Cats vs Dogs binary classifier\n",
    "\n",
    "Now we will use [Transfer learning](http://cs231n.github.io/transfer-learning/) and remove the top layer of a pretrained network [(VGG16 on Imagenet)](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/), extracting the features of the training and validation images and then just training the fully connected top layer of that network. This way we will be able to make accurate predictions even though we just have a small data set.\n",
    "\n",
    "The data conists of 2000 training images (1000 cats and 1000 dogs), 800 validation images (400 cats and 400 dogs), and 100 test images (cats and dogs mixed). [Get the data here.](https://www.dropbox.com/s/a8zo6udq83xsx05/data-x_cnn_data.tar.gz?dl=1) \n",
    "\n",
    "The pretrained VGG16 weighs that you should load into your model (in order to extract the bottleneck features) can be [downloaded here](https://www.dropbox.com/s/fvx7hv5vr8j3sc6/vgg16_weights_features.tar.gz?dl=1). Extracted features are included so that you can run the code instantly.\n",
    "\n",
    "\n",
    "The reason why we are using a pretrained network, extracting bottleneck features and training only the top layers is that this is a great way to obtain a high prediction accuracy without having a huge data set and without having to run the training for a long time. It would require much more data and take up to several days to run this analysis and training your own CNN on a personal computer (in order to obtain the same level of accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "\n",
    "# Part 2: Extract bottleneck features from the data set\n",
    "\n",
    "A good explanation on how this works (rewritten from source: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "#### Using the bottleneck features of a pre-trained network: 90% accuracy in 1 min (GPU) / 10 mins (CPU)\n",
    "\n",
    "We are leveraging the predictive power of a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. The method presented here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![Image of Yaktocat](https://blog.keras.io/img/imgclf/vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal visualization\n",
    "\n",
    "![https://datatoanalytics.files.wordpress.com/2017/04/vgg161.png](https://datatoanalytics.files.wordpress.com/2017/04/vgg161.png)\n",
    "\n",
    "\n",
    "### **Strategy to extract bottleneck features:** \n",
    "\n",
    "We will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will pass our training, validation, and test data through this model once, recording the output (the \"bottleneck features\" from the VGG16 model, ie the output of the last activation maps before the fully-connected layers). \n",
    "\n",
    "The output will be saved as three numpy arrays, and stored on disk as `.npy` files.. The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is how the VGG16 net looks in code\n",
    "You can see the full implementation here of the network we're building and loading in: https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py\n",
    "\n",
    "\n",
    "```python\n",
    "    # VGG 16 model architecture\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Classification block\n",
    "    model.add(Flatten(name='flatten')(x))\n",
    "    model.add(Dense(4096, activation='relu', name='fc1')(x))\n",
    "    model.add(Dense(4096, activation='relu', name='fc2')(x))\n",
    "    model.add(Dense(classes, activation='softmax', name='predictions')(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGEnet Benchmarks (the structure we're using today, 2nd place in 2014)\n",
    "\n",
    "![https://qph.ec.quoracdn.net/main-qimg-fbd17e02f01e60b38ff8ee864c647303](https://cdn-images-1.medium.com/max/800/1*HyaPKtxU07iVzZ4RjJJlUQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving bottleneck features\n",
    "# This can take ~10mins to run\n",
    "\n",
    "#  Run model once to record the bottleneck features using image data generators:\n",
    "\n",
    "def save_bottleneck_features():\n",
    "\n",
    "    from tensorflow.keras import applications\n",
    "    model = applications.vgg16.VGG16(include_top=False, weights='imagenet', \\\n",
    "                                     input_tensor=None, input_shape=(img_width, img_height,3))\n",
    "    \n",
    "    # documentation: https://keras.io/applications/#vgg16\n",
    "    \n",
    "    print('TensorFlow VGG16 model architecture loaded')\n",
    "    # include_top = False, because we drop last layer, then we also only need to\n",
    "    # download weight file that is small\n",
    "    # input_shape with channels last for tensorflow\n",
    "    \n",
    "    # Our original images consist in RGB coefficients in the 0-255 interval, \n",
    "    # but such values would be too high for our models to process (given typical learning rate), \n",
    "    # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    def generate_features(DIR,n_samples,name_str):\n",
    "        '''This is a generator that will read pictures found in\n",
    "        subfolers of 'data/*', and indefinitely generate\n",
    "        batches of rescaled images used to predict\n",
    "        the bottleneck features of the images once\n",
    "        using model.predict_generator(**args**)'''\n",
    "\n",
    "        print('Generate '+name_str+' image features')\n",
    "    \n",
    "        generator = datagen.flow_from_directory(\n",
    "            DIR,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=1,\n",
    "            class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False) # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "        \n",
    "        \n",
    "        features = model.predict_generator(generator, n_samples,verbose=True)\n",
    "        # the predict_generator method returns the output of a model, given\n",
    "        # a generator that yields batches of numpy data\n",
    "        \n",
    "        np.save('features_'+name_str+'.npy', features) # save bottleneck features to file\n",
    "    \n",
    "    generate_features(TEST_DIR, n_test_samples, 'test')\n",
    "    #generate_features(TRAIN_DIR, n_train_samples, 'train')\n",
    "    #generate_features(VAL_DIR, n_validation_samples, 'validation')\n",
    "    \n",
    "    print('\\nDone! Bottleneck features have been saved')\n",
    "\n",
    "\n",
    "print('This has been done before the lecture! Takes many mins to run for all images.')\n",
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra\n",
    "# Obtain class labels and binary classification for validation data\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_gen = datagen.flow_from_directory(VAL_DIR,target_size=(img_width, img_height),\n",
    "                                        batch_size=32,class_mode=None,shuffle=False)\n",
    "\n",
    "val_labels = val_gen.classes\n",
    "\n",
    "print('\\nClassifications:\\n',val_gen.class_indices)\n",
    "print('\\nClass labels:\\n',val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "\n",
    "# Part 3: Train the top layer of your CNN\n",
    "\n",
    "Ones you have extracted and written the bottleneck features to files, read them in again and use them to train the top layer of your network, i.e. the small fully-connected model on top of the stored features. When you have done this record and answer with your prediciton accuracy.\n",
    "\n",
    "**Question:** What is the validation accuracy for the last training epoch, and how is it that we can reach such high accuracy with such small amount of data in a short amount of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in bottleneck features\n",
    "# Run the code below to train your CNN with the training data\n",
    "\n",
    "train_data = np.load('features_train.npy')\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * (n_train_samples // 2) + [1] * (n_train_samples // 2))\n",
    "\n",
    "validation_data = np.load('features_validation.npy')\n",
    "# same as val_labels above\n",
    "validation_labels = np.array([0] * (n_validation_samples // 2) + [1] * (n_validation_samples // 2))\n",
    "\n",
    "# Add top layers trained ontop of extracted VGG features\n",
    "# Small fully connected model trained on top of the stored features\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "#We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. \n",
    "#To go with it we will also use the binary_crossentropy loss to train our model.\n",
    "\n",
    "'''\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "MODEL_WEIGHTS_FILE = 'vgg16-best-weights.h5'\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_accuracy', verbose=1, save_best_only=True)]\n",
    "\n",
    "history = model.fit(train_data, train_labels, verbose=1, \\\n",
    "                    nb_epoch=20, batch_size=32, \\\n",
    "                    validation_data=(validation_data, validation_labels),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# Save weights to disk\n",
    "\n",
    "# Save model architecture to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"mod_appendix.json\", \"w\") as json_file: # save model\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"catvsdogs_VGG16_pretrained_tf_top.h5\") # save weights\n",
    "print(\"Saved model to disk\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.model.load_weights('vgg16-best-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = history.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # only the last layer hsa 2Mn weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': range(1,n_epoch+1),\n",
    "                    'training': history.history['accuracy'],\n",
    "                    'validation': history.history['val_accuracy']})\n",
    "ax = acc.plot(x='epoch', figsize=(10,6), grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.7,1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "\n",
    "# Part 4: Validate accuracy and make predictions on unlabeled data\n",
    "\n",
    "**Question:** First use the model trained in Part 3 to determine the accuracy on the validation data set (is it the same as one the last training epoch?\n",
    "\n",
    "Lastly, use the model trained in Part 3 to classify the test data images. I.e., create a function that loads one image from the test data and then predicts if it is a cat or a dog and with what probability it thinks it is a cat or a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "print('Model accuracy on validation set:',model.evaluate(validation_data,val_labels,verbose=0)[1]*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print try images:\n",
    "\n",
    "# Use the model trained in Problem 1 to classify the test data images.\n",
    "# Create a function that loads one image from the test data and then predicts\n",
    "# if it is a cat or a dog and with what probability it thinks it is a cat or a dog\n",
    "#\n",
    "# Use variable test_data to make predictions\n",
    "# Use list test_images to obtain the file name for all images (Note: test_images[0] corresponds to test_data[0])\n",
    "# Use function plot_pic(img) to plot the image file\n",
    "\n",
    "## Load in processed images feature to feed into bottleneck model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "test_data = np.load('features_test.npy')\n",
    "\n",
    "test_images =  [TEST_DIR+'catvdog/'+img for img in sorted(os.listdir(TEST_DIR+'catvdog/'))]\n",
    "\n",
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    im = np.array(Image.open(file_path))\n",
    "    return im\n",
    "\n",
    "def plot_pic(img):\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mod,i=0,r=None):\n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        class_pred = mod.predict_classes(test_data,verbose=0)[idx]\n",
    "        prob_pred = mod.predict_proba(test_data,verbose=0)[idx]\n",
    "        \n",
    "        if class_pred ==0:\n",
    "            prob_pred = 1-prob_pred\n",
    "            class_guess='CAT'\n",
    "        else:\n",
    "            class_guess='DOG'\n",
    "        \n",
    "        print('\\n\\nI think this is a ' + class_guess + ' with ' +str(round(float(prob_pred)*100,5)) + '% probability')\n",
    "        if test_images[idx]=='./data/test/catvdog/.DS_Store' or '.ipynb_checkpoints' in test_images[idx]:\n",
    "            continue\n",
    "        plot_pic(test_images[idx])\n",
    "\n",
    "#predict(model,r=range(0,10))       \n",
    "predict(model,r=range(88,len(test_images))) # seems to be doing really well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into fine tuning a network and freezing layers (to get to 97% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing how conv nets learn\n",
    "Intermediate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('cats_and_dogs_small_2.h5')\n",
    "model.summary()  # As a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = TRAIN_DIR+'cats/cat0003.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We preprocess the image into a 4D tensor\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a list of 5 Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "\n",
    "# Part 5 -- right now built for Theano\n",
    "\n",
    "Redo the model and pipeline created in Part 1-4 in order to make predictions on the 50 image classes. Note that you might have to change how you read in the images, so that when you train the model you do a cross validation split (25 / 75) instead of specifying a specific validation set.  And, you will want to use a `softmax` activation layer instead of a `sigmoid` one (to do multiclass classification).\n",
    "\n",
    "The data can be downloaded here: https://www.dropbox.com/s/suy8u0hnthwr2su/50_categories.tar.gz?dl=1\n",
    "\n",
    "Note that you do not have to additional data to make predictions on data not used in the training (however you can easily download 3-5 images like that from Google to try your model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at files, note all cat images and dog images are unique\n",
    "for path, dirs, files in os.walk('./50_categories'):\n",
    "    print('FOLDER',path)\n",
    "    for f in files[:2]:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = next(os.walk('./50_categories'))\n",
    "print(categories)\n",
    "categories = categories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categories to an integer\n",
    "cat_dic = dict()\n",
    "for idx, cat in enumerate(categories):\n",
    "    cat_dic[cat] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs = 0\n",
    "for cat in categories:\n",
    "    nbr_cat_imgs = len(next(os.walk('./50_categories/'+cat))[2])\n",
    "    print('Number of '+cat+' images:', nbr_cat_imgs)\n",
    "    n_imgs+=nbr_cat_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "len(glob(\"./50_categories/*/*.jpg\")) #4244 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with opencv\n",
    "img = cv2.imread('./50_categories/airplanes/airplanes_0001.jpg')\n",
    "img.shape\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_images = glob(\"./50_categories/*/*.jpg\")\n",
    "path_all_images[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_categories(images):\n",
    "    \"\"\"Get the true categories of a set of paths to images, based on the\n",
    "    directory they are located in.\n",
    "\n",
    "    The paths should have the form:\n",
    "        path/to/image/category/image.jpg\n",
    "\n",
    "    Where the image filename is the last item in the path, and the\n",
    "    directory (category name) is the second to last item in the path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list\n",
    "        List of paths to images\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    categories : numpy.ndarray\n",
    "        An array of integers in order of the images, corresponding to\n",
    "        each image's category\n",
    "    category_map : list\n",
    "        A list of category names. The category integers in\n",
    "        `categories` are indices into this list.\n",
    "\n",
    "    \"\"\"\n",
    "    get_category = lambda x: os.path.split(os.path.split(x)[0])[1]\n",
    "    categories = list(map(get_category, images))\n",
    "    category_map = sorted(set(categories))\n",
    "    categories = np.array(map(category_map.index, categories))\n",
    "    return categories, category_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cats, img_cat_map = get_image_categories(path_all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cat_map[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "DATA_DIR = './50_categories/'\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "n_samples = n_imgs\n",
    "n_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Run model once to record the bottleneck features using image data generators:\n",
    "#  Note: This can take a lot of time\n",
    "\n",
    "def save_bottleneck_features():\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists('vgg16_weights.h5'), 'Model weights not found (Download file vgg16_weights.h5 from bcourses).'\n",
    "    f = h5py.File('vgg16_weights.h5')\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "    \n",
    "    # Rescale value we multiply the data before any other processing. \n",
    "    # Our original images consist in RGB coefficients in the 0-255, \n",
    "    # but such values would be too high for our models to process (given typical learning rate), \n",
    "    # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    def generate_features(DIR,n_samples,name_str):\n",
    "        '''\n",
    "        This is a generator that will read pictures found in\n",
    "        subfolers of 'data/*', and indefinitely generate\n",
    "        batches of image rescaled images used to predict\n",
    "        the bottleneck features of the images once\n",
    "        using model.predict_generator(**args**)\n",
    "       '''\n",
    "        print('Generate '+name_str+' image features')\n",
    "\n",
    "        generator = datagen.flow_from_directory(\n",
    "            DIR,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False)  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "\n",
    "        \n",
    "        features = model.predict_generator(generator, n_samples)\n",
    "        # the predict_generator method returns the output of a model, given\n",
    "        # a generator that yields batches of numpy data\n",
    "        \n",
    "        np.save(open('50_classes_features.npy', 'w'), features) # save bottleneck features to file\n",
    "        \n",
    "    generate_features(DATA_DIR, n_samples, 'data')\n",
    "\n",
    "\n",
    "    \n",
    "save_bottleneck_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain image labels and binary classification\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "class_gen = datagen.flow_from_directory(DATA_DIR,target_size=(img_width, img_height),\n",
    "                                        batch_size=32,class_mode=None,shuffle=False)\n",
    "\n",
    "class_labels = class_gen.classes\n",
    "\n",
    "print('\\nClassifications:\\n',class_gen.class_indices)\n",
    "print('\\nClass labels:\\n',class_labels)\n",
    "\n",
    "print(class_gen.class_indices.keys() == cat_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all of our features are stored in order\n",
    "# and we have not split up our training data into training and validation folders\n",
    "# in order to not only train on the first classes we need to randomize our samples\n",
    "# this can easily be done with scikit-learn's test_train_split module\n",
    "\n",
    "# we train on the X_train and y_train sets\n",
    "# then we evaluate our model on the X_test and y_test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "class_data = np.load('50_classes_features.npy') # load in bottleneck features\n",
    "print(class_data.shape)\n",
    "X_train, X_test, y_train, y_test, path_train, path_test = train_test_split(class_data, class_labels, path_all_images,\\\n",
    "                                                                           test_size=0.2, random_state=150)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_train[0:4])\n",
    "print([img_cat_map[i] for i in y_train[0:4]]) # the img_paths have been mapped correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in bottleneck features\n",
    "# Run the code below to train your CNN with the training data\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "\n",
    "\n",
    "# Add top layers trained ontop of extracted VGG features\n",
    "# Small fully connected model trained on top of the stored features\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=class_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='softmax'))\n",
    "#model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(50, activation='softmax'))\n",
    "\n",
    "    \n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, nb_epoch=40,validation_split=0, verbose=1) # we don't need a validation split\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_pred_class = model.predict_classes(X_test,verbose=1)\n",
    "#val_pred_prob = model.predict_proba(class_data,verbose=0)\n",
    "\n",
    "print(model.evaluate(X_test,y_test,verbose=0))\n",
    "# First number is validation loss, loss of the objective function\n",
    "# Second number validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(val_pred_class)) #ok 50 classes in our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(val_pred_class==y_test)/len(y_test) # ~60% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(X_test,y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    #return cv2.resize(img, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n",
    "def plot_pic(img):\n",
    "    # Plot openCV pic\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative plotpic function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_pic(img):\n",
    "    image = mpimg.imread(img)\n",
    "    plt.figure(figsize=(9,9))\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(mod,i=0,r=None):\n",
    "    preds = mod.predict_classes(X_test,verbose=0)\n",
    "    \n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        \n",
    "        img_path = path_test[idx]\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        #x = image.img_to_array(img)\n",
    "        #x = np.expand_dims(x, axis=0)\n",
    "        class_pred = preds[idx]\n",
    "        \n",
    "        class_guess = img_cat_map[class_pred]\n",
    "        \n",
    "        print('\\n\\nProbable category: ' + class_guess)\n",
    "        plot_pic(path_test[idx])\n",
    "\n",
    "predict(model,r=range(0,100))\n",
    "\n",
    "# As we can see below it is pretty accurate, however, for the case when we don't have many \n",
    "# training samples the accuracy is not as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
