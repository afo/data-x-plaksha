{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png\" align=\"left\"></img><br><br><br><br>\n",
    "\n",
    "\n",
    "## Breakout Lecture 8: Web scraping & web crawling\n",
    "\n",
    "**Author List**: Alexander Fred Ojala\n",
    "\n",
    "**Original Sources**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ & https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "**License**: Feel free to do whatever you want to with this code\n",
    "\n",
    "**Compatibility:** Python 2.x and 3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "(Clickable document links)\n",
    "___\n",
    "\n",
    "### [0: Pre-steup](#sec0)\n",
    "Document setup and Python 2 and Python 3 compability\n",
    "\n",
    "### [1: Simple webscrpaing intro](#sec1)\n",
    "\n",
    "Simple example of webscraping on a premade HTML template\n",
    "\n",
    "### [2: Scrape Data-X Schedule](#sec2)\n",
    "\n",
    "Find and scrape the current Data-X schedule. \n",
    "\n",
    "### [3: Scrape Images and Files](#sec3)\n",
    "\n",
    "Scrape a website of Images, PDF's, CSV data or any other file type.\n",
    "\n",
    "## [Breakout Problem: Scrape Weather Data](#sec4)\n",
    "\n",
    "Scrape real time weather data in Berkeley.\n",
    "\n",
    "\n",
    "### [Appendix](#sec5)\n",
    "\n",
    "#### [Scrape Bloomberg sitemap for political news headlines](#sec6)\n",
    "\n",
    "#### [Webcrawl Twitter, recusrive URL link fetcher + depth](#sec7)\n",
    "\n",
    "#### [SEO, visualize webite categories as a tree](#sec8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "## Pre-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\")) # if 100% it would fit the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# Webscraping intro\n",
    "\n",
    "In order to scrape content from a website we first need to download the HTML contents of the website. This can be done with the Python library **requests** (with its `.get` method).\n",
    "\n",
    "Then when we want to extract certain information from a website we use the scraping tool **BeautifulSoup4** (import bs4). In order to extract information with beautifulsoup we have to create a soup object from the HTML source code of a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # The requests library is an HTTP library for getting content and posting etc.\n",
    "import bs4 as bs # BeautifulSoup4 is a Python library for pulling data out of HTML and XML code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a simple website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = requests.get(\"https://alexanderfo.github.io\") # a GET request will download the HTML webpage.\n",
    "print(source) # If <Response [200]> then the website has been downloaded succesfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different types of repsonses:**\n",
    "Generally status code starting with 2 indicates success. Status code starting with 4 or 5 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(source.content) # This is the HTML content of the website, as you can see it's quite hard to decipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(source.content)) # type byte in Python 3, type str in Python 2. Byte is default encoding of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in source.content to beautifulsoup \n",
    "# beautifulsoup can parse (extract specific information) HTML code\n",
    "\n",
    "soup = bs.BeautifulSoup(source.content ,features='lxml') # we pass in the source and choose a parser \n",
    "\n",
    "# features specifies what type of code we are parsing, here 'lxml' specifies an HTML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup) # This is the HTML code of the website, decoded as a beautiful soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppose we want to extract content that is shown on the website\n",
    "\n",
    "print(soup.body) # This is the main content of the website, located within the <body> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.title) # Title of the website\n",
    "print(soup.find('title')) # same as .title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to extract specific text\n",
    "print(soup.find('p')) # will only return first <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.find('p').text) # extracts the string within the <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we want to extract all <p> tags\n",
    "print(soup.find_all('p')) # returns list of all <p> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.find(class_='header')) # we can also search for classes within all tags, using class_\n",
    "print(soup.find(id='second'))\n",
    "# note _ is used to distinguish with Python's builtin class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.find_all(class_='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'): # print all p tags in the list\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract links / urls\n",
    "# Links in html is usually coded as <a href=\"url\"> where the link is url\n",
    "\n",
    "print(soup.a)\n",
    "print(type(soup.a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if we only want the link\n",
    "attendance_link = soup.find('a').get('href') # we want to get the string specified by the 'href inside the a tag\n",
    "print(\"To record attendance for today's lecture go to: \",attendance_link) # then we have extracted the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "\n",
    "# Scrape the current Syllabus Schedule from the Data-X website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = requests.get('https://data-x.blog/').content # get the source content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = bs.BeautifulSoup(source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()) # .prettify() method makes the HTML code more readable\n",
    "\n",
    "# as you can see this code is more difficult to read then the simple example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.find('title').text) # we are at the correct website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "navigation_bar = soup.find('nav')\n",
    "print(navigation_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we want to find the Syllabus, however we are at the root web page, not displaying the syllabus\n",
    "# Get links from the data-x website\n",
    "for url in navigation_bar.find_all('a'): # look for links in the navigation bar. Tag <nav>\n",
    "    link = url.get('href')\n",
    "    if 'data-x.blog' in link:\n",
    "        print(link) # we see that the syllabus is located at the url https://data-x.blog/syllabus-data-x/\n",
    "        if 'syllabus' in link:\n",
    "            syllabus_url = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(syllabus_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open new connection to the syllabus url. Replace soup object.\n",
    "source = requests.get(syllabus_url).content\n",
    "soup = bs.BeautifulSoup(source, 'lxml') # 'lxml' parser better for tables, very similar to 'html.parser'\n",
    "\n",
    "print(soup.body.prettify()) # we can see that the table is stored within <td> tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the course scheudle table\n",
    "Usually data on a website is stored in tables under the `<td>` tag. Here we want to extract the information in the Data-X syllabus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also get the table\n",
    "table = soup.find('table')\n",
    "print(table.prettify()) #HTML code of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A new row in an HTML table starts with <tr> tag\n",
    "# A new column entry is defined by <td> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_result = list()\n",
    "for row in table.find_all('tr'):\n",
    "    row_cells = row.find_all('td') # find all table data\n",
    "    row_entries = [cell.text for cell in row_cells]\n",
    "    print(row_entries) \n",
    "    table_result.append(row_entries)# get all the table data into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also read it in to a Pandas DataFrame\n",
    "import pandas as pd    \n",
    "df = pd.DataFrame(table_result)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas can also grab tables from a website automatically\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# requires html5lib: \n",
    "#!conda install --yes html5lib\n",
    "dfs = pd.read_html('https://data-x.blog/syllabus-data-x/',header=0) # returns a list of all tables at url\n",
    "# header = 0, indicates that first row is header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(dfs)) #list of tables\n",
    "print(len(dfs)) # we only have one table\n",
    "print(type(dfs[0])) # stored as DataFrame\n",
    "df = dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks great\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "# Scrape images and other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As we can see there are two images on the data-x syllabus site that we might want to download\n",
    "# Images are displayed with the <img> tag in HTML\n",
    "\n",
    "print(soup.find('img')) # as we can see below the image urls are stored as the src inside the img tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse all url to the images\n",
    "img_urls = list()\n",
    "for img in soup.find_all('img'): \n",
    "    img_url = img.get('src') \n",
    "    print(img_url) # we only want images with .jpg extension\n",
    "    if '.jpg' in img_url:\n",
    "        img_urls.append(img_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To downloads and save files with Python we can use the shutil library\n",
    "# which is a file operations library\n",
    "\n",
    "import shutil\n",
    "\n",
    "for idx, img_url in enumerate(img_urls): #enumarte to create a file integer name for every image\n",
    "    \n",
    "    img_source = requests.get(img_url, stream=True) \n",
    "    # we set stream = True to download/ stream the content of the data\n",
    "    \n",
    "    with open('img'+str(idx)+'.jpg', 'wb') as file: # open file connection, create file and write to it\n",
    "        shutil.copyfileobj(img_source.raw, file) # save the raw file object\n",
    "\n",
    "    del img_source # to remove the file from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function to download files of any type from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or a for file links)\n",
    "    source_tag = the source tag for the file url (usually src for images or href for files)\n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    max = integer (max number of files to scrape, if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "\n",
    "    source = requests.get(url).content\n",
    "    soup = bs.BeautifulSoup(source,'lxml')\n",
    "    \n",
    "    i=0\n",
    "    for link in soup.find_all(html_tag):\n",
    "        file_url=link.get(source_tag)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "\n",
    "            if file_type in file_url: #only check for specific file type\n",
    "\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and write to it\n",
    "                    shutil.copyfileobj(file_source.raw, file) # save the raw file object\n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('EXCLUDED:',file_url) # urls not downloaded from\n",
    "                \n",
    "        if i==max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "py_file_scraper('https://data-x.blog/syllabus-data-x/') # scrape images form data-x syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape pdf's from data-x site\n",
    "py_file_scraper('https://data-x.blog/',html_tag='a',source_tag='href',file_type='.pdf',max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scrape csv files from website\n",
    "py_file_scraper('http://www-eio.upc.edu/~pau/cms/rdata/datasets.html',html_tag='a', # R data sets\n",
    "                source_tag='href', file_type='.csv',max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='sec4'></a>\n",
    "# Breakout problem\n",
    "\n",
    "\n",
    "In this week's breakout you should extract live weather data in Berkeley from:\n",
    "\n",
    "[http://forecast.weather.gov/MapClick.php?lat=37.87158815800046&lon=-122.27274583799971](http://forecast.weather.gov/MapClick.php?lat=37.87158815800046&lon=-122.27274583799971)\n",
    "\n",
    "* Task scrape\n",
    "    * period / day (as Tonight, Friday, FridayNight etc.\n",
    "    * the temperature for the period (as Low, High)\n",
    "    * the long weather description (e.g. Partly cloudy, with a low around 49..)\n",
    "    \n",
    "Store the scraped data strings in a Pandas DataFrame\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The weather information is found in a div tag with `id='seven-day-forecast'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Breakout solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "# Scrape Bloomberg sitemap (XML) for current political news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps\n",
    "\n",
    "# Before scraping a website look at robots.txt file\n",
    "bs.BeautifulSoup(requests.get('https://www.bloomberg.com/robots.txt').content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = requests.get('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').content\n",
    "soup = bs.BeautifulSoup(source,'xml') # Note parser 'xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find political news headlines\n",
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    #print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "# Web crawl\n",
    "\n",
    "Web crawling is almost like webscraping, but instead you crawl a specific website (and often its subsites) and extract meta information. It can be seen as simple, recursive scraping. This can be used for web indexing (in order to build a web search engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawl Twitter account\n",
    "**Authors:** Kunal Desai & Alexander Fred Ojala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to maintain the urls and the number of times they appear\n",
    "\n",
    "url_dict = dict()\n",
    "\n",
    "def add_to_dict(url_d, key):\n",
    "    if key in url_d:\n",
    "        url_d[key] = url_d[key] + 1\n",
    "    else:\n",
    "        url_d[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recursive function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and \"https://\" in link['href']:\n",
    "#             print(link['href'])\n",
    "            add_to_dict(url_dict, link['href'])\n",
    "            get_urls(link['href'], depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterative function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls_iterative(url, depth):\n",
    "    urls = [url]\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and \"https://\" in link['href']:\n",
    "                add_to_dict(url_dict, link['href'])\n",
    "                urls.append(link['href'])\n",
    "        if len(urls) > depth:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_urls(\"https://twitter.com/GolfWorld\", 2)\n",
    "for key in url_dict:\n",
    "    print(str(key) + \"  ----   \" + str(url_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec8'></a>\n",
    "# SEO: Visualize sitemap and categories in a website\n",
    "\n",
    "**Source:** https://www.ayima.com/guides/how-to-visualize-an-xml-sitemap-using-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize XML sitemap with categories!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.sportchek.ca/sitemap.xml'\n",
    "url = 'https://www.bloomberg.com/feeds/bpol/sitemap_index.xml'\n",
    "page = requests.get(url)\n",
    "print('Loaded page with: %s' % page)\n",
    "\n",
    "sitemap_index = BeautifulSoup(page.content, 'html.parser')\n",
    "print('Created %s object' % type(sitemap_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = [element.text for element in sitemap_index.findAll('loc')]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    ''' Open an XML sitemap and find content wrapped in loc tags. '''\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = [element.text for element in soup.findAll('loc')]\n",
    "\n",
    "    return links\n",
    "\n",
    "sitemap_urls = []\n",
    "for url in urls:\n",
    "    links = extract_links(url)\n",
    "    sitemap_urls += links\n",
    "\n",
    "print('Found {:,} URLs in the sitemap'.format(len(sitemap_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sitemap_urls.dat', 'w') as f:\n",
    "    for url in sitemap_urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Categorize a list of URLs by site path.\n",
    "The file containing the URLs should exist in the working directory and be\n",
    "named sitemap_urls.dat. It should contain one URL per line.\n",
    "Categorization depth can be specified by executing a call like this in the\n",
    "terminal (where we set the granularity depth level to 5):\n",
    "    python categorize_urls.py --depth 5\n",
    "The same result can be achieved by setting the categorization_depth variable\n",
    "manually at the head of this file and running the script with:\n",
    "    python categorize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "categorization_depth=3\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "\n",
    "def peel_layers(urls, layers=3):\n",
    "    ''' Builds a dataframe containing all unique page identifiers up\n",
    "    to a specified depth and counts the number of sub-pages for each.\n",
    "    Prints results to a CSV file.\n",
    "    urls : list\n",
    "        List of page URLs.\n",
    "    layers : int\n",
    "        Depth of automated URL search. Large values for this parameter\n",
    "        may cause long runtimes depending on the number of URLs.\n",
    "    '''\n",
    "\n",
    "    # Store results in a dataframe\n",
    "    sitemap_layers = pd.DataFrame()\n",
    "\n",
    "    # Get base levels\n",
    "    bases = pd.Series([url.split('//')[-1].split('/')[0] for url in urls])\n",
    "    sitemap_layers[0] = bases\n",
    "\n",
    "    # Get specified number of layers\n",
    "    for layer in range(1, layers+1):\n",
    "\n",
    "        page_layer = []\n",
    "        for url, base in zip(urls, bases):\n",
    "            try:\n",
    "                page_layer.append(url.split(base)[-1].split('/')[layer])\n",
    "            except:\n",
    "                # There is nothing that deep!\n",
    "                page_layer.append('')\n",
    "\n",
    "        sitemap_layers[layer] = page_layer\n",
    "\n",
    "    # Count and drop duplicate rows + sort\n",
    "    sitemap_layers = sitemap_layers.groupby(list(range(0, layers+1)))[0].count()\\\n",
    "                     .rename('counts').reset_index()\\\n",
    "                     .sort_values('counts', ascending=False)\\\n",
    "                     .sort_values(list(range(0, layers)), ascending=True)\\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "    # Convert column names to string types and export\n",
    "    sitemap_layers.columns = [str(col) for col in sitemap_layers.columns]\n",
    "    sitemap_layers.to_csv('sitemap_layers.csv', index=False)\n",
    "\n",
    "    # Return the dataframe\n",
    "    return sitemap_layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sitemap_urls = open('sitemap_urls.dat', 'r').read().splitlines()\n",
    "print('Loaded {:,} URLs'.format(len(sitemap_urls)))\n",
    "\n",
    "print('Categorizing up to a depth of %d' % categorization_depth)\n",
    "sitemap_layers = peel_layers(urls=sitemap_urls,\n",
    "                             layers=categorization_depth)\n",
    "print('Printed {:,} rows of data to sitemap_layers.csv'.format(len(sitemap_layers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize a list of URLs by site path.\n",
    "This script reads in the sitemap_layers.csv file created by the\n",
    "categorize_urls.py script and builds a graph visualization using Graphviz.\n",
    "Graph depth can be specified by executing a call like this in the\n",
    "terminal:\n",
    "    python visualize_urls.py --depth 4 --limit 10 --title \"My Sitemap\" --style \"dark\" --size \"40\"\n",
    "The same result can be achieved by setting the variables manually at the head\n",
    "of this file and running the script with:\n",
    "    python visualize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# Set global variables\n",
    "\n",
    "graph_depth = 3  # Number of layers deep to plot categorization\n",
    "limit = 3       # Maximum number of nodes for a branch\n",
    "title = ''       # Graph title\n",
    "style = 'light'  # Graph style, can be \"light\" or \"dark\"\n",
    "size = '8,5'     # Size of rendered PDF graph\n",
    "\n",
    "\n",
    "# Import external library dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "def make_sitemap_graph(df, layers=3, limit=50, size='8,5'):\n",
    "    ''' Make a sitemap graph up to a specified layer depth.\n",
    "    sitemap_layers : DataFrame\n",
    "        The dataframe created by the peel_layers function\n",
    "        containing sitemap information.\n",
    "    layers : int\n",
    "        Maximum depth to plot.\n",
    "    limit : int\n",
    "        The maximum number node edge connections. Good to set this\n",
    "        low for visualizing deep into site maps.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Check to make sure we are not trying to plot too many layers\n",
    "    if layers > len(df) - 1:\n",
    "        layers = len(df)-1\n",
    "        print('There are only %d layers available to plot, setting layers=%d'\n",
    "              % (layers, layers))\n",
    "\n",
    "\n",
    "    # Initialize graph\n",
    "    f = graphviz.Digraph('sitemap', filename='sitemap_graph_%d_layer' % layers)\n",
    "    f.body.extend(['rankdir=LR', 'size=\"%s\"' % size])\n",
    "\n",
    "\n",
    "    def add_branch(f, names, vals, limit, connect_to=''):\n",
    "        ''' Adds a set of nodes and edges to nodes on the previous layer. '''\n",
    "\n",
    "        # Get the currently existing node names\n",
    "        node_names = [item.split('\"')[1] for item in f.body if 'label' in item]\n",
    "\n",
    "        # Only add a new branch it it will connect to a previously created node\n",
    "        if connect_to:\n",
    "            if connect_to in node_names:\n",
    "                for name, val in list(zip(names, vals))[:limit]:\n",
    "                    f.node(name='%s-%s' % (connect_to, name), label=name)\n",
    "                    f.edge(connect_to, '%s-%s' % (connect_to, name), label='{:,}'.format(val))\n",
    "\n",
    "\n",
    "    f.attr('node', shape='rectangle') # Plot nodes as rectangles\n",
    "\n",
    "    # Add the first layer of nodes\n",
    "    for name, counts in df.groupby(['0'])['counts'].sum().reset_index()\\\n",
    "                          .sort_values(['counts'], ascending=False).values:\n",
    "        f.node(name=name, label='{} ({:,})'.format(name, counts))\n",
    "\n",
    "    if layers == 0:\n",
    "        return f\n",
    "\n",
    "    f.attr('node', shape='oval') # Plot nodes as ovals\n",
    "    f.graph_attr.update()\n",
    "\n",
    "    # Loop over each layer adding nodes and edges to prior nodes\n",
    "    for i in range(1, layers+1):\n",
    "        cols = [str(i_) for i_ in range(i)]\n",
    "        nodes = df[cols].drop_duplicates().values\n",
    "        for j, k in enumerate(nodes):\n",
    "\n",
    "            # Compute the mask to select correct data\n",
    "            mask = True\n",
    "            for j_, ki in enumerate(k):\n",
    "                mask &= df[str(j_)] == ki\n",
    "\n",
    "            # Select the data then count branch size, sort, and truncate\n",
    "            data = df[mask].groupby([str(i)])['counts'].sum()\\\n",
    "                    .reset_index().sort_values(['counts'], ascending=False)\n",
    "\n",
    "            # Add to the graph\n",
    "            add_branch(f,\n",
    "                       names=data[str(i)].values,\n",
    "                       vals=data['counts'].values,\n",
    "                       limit=limit,\n",
    "                       connect_to='-'.join(['%s']*i) % tuple(k))\n",
    "\n",
    "            print(('Built graph up to node %d / %d in layer %d' % (j, len(nodes), i))\\\n",
    "                    .ljust(50), end='\\r')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_style(f, style, title=''):\n",
    "    ''' Apply the style and add a title if desired. More styling options are\n",
    "    documented here: http://www.graphviz.org/doc/info/attrs.html#d:style\n",
    "    f : graphviz.dot.Digraph\n",
    "        The graph object as created by graphviz.\n",
    "    style : str\n",
    "        Available styles: 'light', 'dark'\n",
    "    title : str\n",
    "        Optional title placed at the bottom of the graph.\n",
    "    '''\n",
    "\n",
    "    dark_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'bgcolor': '#3a3a3a',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'white',\n",
    "            'fillcolor': 'black',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'white',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'white',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    light_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'black',\n",
    "            'fillcolor': '#dbdddd',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'black',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'black',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if style == 'light':\n",
    "        apply_style = light_style\n",
    "\n",
    "    elif style == 'dark':\n",
    "        apply_style = dark_style\n",
    "\n",
    "    f.graph_attr = apply_style['graph']\n",
    "    f.node_attr = apply_style['nodes']\n",
    "    f.edge_attr = apply_style['edges']\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read in categorized data\n",
    "sitemap_layers = pd.read_csv('sitemap_layers.csv', dtype=str)\n",
    "# Convert numerical column to integer\n",
    "sitemap_layers.counts = sitemap_layers.counts.apply(int)\n",
    "print('Loaded {:,} rows of categorized data from sitemap_layers.csv'\\\n",
    "        .format(len(sitemap_layers)))\n",
    "\n",
    "print('Building %d layer deep sitemap graph' % graph_depth)\n",
    "f = make_sitemap_graph(sitemap_layers, layers=graph_depth,\n",
    "                       limit=limit, size=size)\n",
    "f = apply_style(f, style=style, title=title)\n",
    "\n",
    "f.render(cleanup=True)\n",
    "print('Exported graph to sitemap_graph_%d_layer.pdf' % graph_depth)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
