{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png\"></img><br><br>\n",
    "\n",
    "\n",
    "\n",
    "## Notebook: Web Scraping & Web Crawling\n",
    "\n",
    "**Author List**: Alexander Fred-Ojala, Ishaan Malhi, Sudarshan Gopalakrishnan\n",
    "\n",
    "**Original Sources**: \n",
    "* https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "**License**: Feel free to do whatever you want to with this code\n",
    "\n",
    "**Compatibility:** Python >= 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are learning today\n",
    "\n",
    "- Data is seldom all clean and readily available\n",
    "\n",
    "- Often times you might need to gather data from websites because of any or all of the following:\n",
    "\n",
    " - You need to update your data regularly (either as a live stream or in batch, i.e every hour, day etc)\n",
    " - Data isn't readily available, you need to collect data for your tasks\n",
    " - Because it's fun and why not do it when you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    " - Web scraping is an important arsenal to have in your toolkit if you want to gather different types of data.\n",
    " - Usually it's a part of the \"Data Gathering\" stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once you gather your data, you clean, prepare/preprocess and use it in your tasks (prediction, analysis etc)\n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/mG1MxDDEMSAVkF7da3/giphy.gif\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Popular Web Scraping tools & libraries\n",
    "\n",
    "This notebook mainly goes over how to get data with the Python packages `requests` and  `BeautifulSoup`. However, there are many other Python packages that can be used for scraping.\n",
    "\n",
    "Two very popular and widely used are:\n",
    "\n",
    "* **[Selenium:](http://selenium-python.readthedocs.io/)** Pyton scraper that can act as a human when visiting websites, almost like a macro. Makes sense of modern Javascript based websites built with React, Angular etc.\n",
    "* **[Scrapy:](https://scrapy.org/)** For automated scripting and has a lot of built in tools for web crawling and scraping that can facilitate the process (e.g. time based, IP rotation etc). Mainly script based scraping for larger projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why do you think a library like `requests` and `BeautifulSoup` can't scrape websites using Frontend Web Apps (like React, Angular, Ember etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The `requests` library asks for static (html) content. Frontend apps (Angular, React, Ember.js etc) dynamically create/load content on the fly. That itself has led to their popularity since you don't need to render everything from scratch.\n",
    "\n",
    "Fun side note: Frontend web apps also break search engine indexing. If we can't scrape the full site, so can't most search engine bots. A workaround to this is called `Server Side Rendering`. It's an interesting way to \"run\" a frontend web app on the backend and give back static content. Most sites do this nowadays but this is a caveat you should keep in mind when scraping websites.\n",
    "\n",
    "An interesting application in industry is [how Airbnb does this](https://github.com/airbnb/hypernova)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: Application Programming Interfaces\n",
    "\n",
    "Many services offer API's to grab data (Twitter, Wikipedia, Reddit etc.) You can find an example of how to extract data from an API in the Pandas notebook when we obtain stock data in CSV format to do analysis. If a good API exists, it is usually the preferred method of obtaining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs vs Web Scraping\n",
    "\n",
    "Sometimes APIs don't give us everything we need, OR we need to gather data from websites that don't have an API. In this case, we use Web Scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful webscraping Cheat Sheet\n",
    "\n",
    "If you want a good documentation of functions in requests and Beautifulsoup (as well as how to save scarped data to an SQLite database), this is a good resource:\n",
    "\n",
    "- https://blog.hartleybrody.com/web-scraping-cheat-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "(Clickable document links)\n",
    "___\n",
    "\n",
    "### [0: Pre-steup](#sec0)\n",
    "Document setup and Python 2 and Python 3 compability\n",
    "\n",
    "### [1: Simple webscraping intro](#sec1)\n",
    "\n",
    "Simple example of webscraping on a premade HTML template\n",
    "\n",
    "### [2: Scrape Data-X Schedule](#sec2)\n",
    "\n",
    "Find and scrape the current Data-X schedule. \n",
    "\n",
    "### [3: IMDB top 250 movies w MetaScore](#sec3)\n",
    "\n",
    "Scrape IMDB and compare MetaScore to user reviews.\n",
    "\n",
    "### [4: Scrape Images and Files](#sec4)\n",
    "\n",
    "Scrape a website of Images, PDF's, CSV data or any other file type.\n",
    "\n",
    "## [Breakout Problem: Scrape Weather Data](#secBK)\n",
    "\n",
    "Scrape real time weather data in Berkeley.\n",
    "\n",
    "\n",
    "### [Appendix](#sec5)\n",
    "\n",
    "#### [Scrape Bloomberg sitemap for political news headlines](#sec6)\n",
    "\n",
    "#### [Webcrawl Twitter, recusrive URL link fetcher + depth](#sec7)\n",
    "\n",
    "#### [SEO, visualize webite categories as a tree](#sec8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec0'></a>\n",
    "## Pre-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\")) \n",
    "# if 100% it would fit the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "# Webscraping intro\n",
    "\n",
    "In order to scrape content from a website we first need to download the HTML contents of the website. This can be done with the Python library **requests** (with its `.get` method).\n",
    "\n",
    "Then when we want to extract certain information from a website we use the scraping tool **BeautifulSoup4** (import bs4). In order to parse information with beautifulsoup we have to create a soup object from the HTML source code of a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # The requests library is an \n",
    "# HTTP library for getting and posting content etc.\n",
    "\n",
    "import bs4 as bs # BeautifulSoup4 is a Python library \n",
    "# for pulling data out of HTML and XML code.\n",
    "# We can query markup languages for specific content\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a simple website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get(\"https://alex.fo/other/data-x/\") \n",
    "# a GET request will download the HTML webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If <Response [200]> then \n",
    "# the website has been downloaded succesfully\n",
    "\n",
    "source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different types of repsonses:**\n",
    "Generally status code starting with 2 indicates success. Status codes starting with 4 or 5 indicates error. Frequent appearances of status codes like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the HTML content of the website,\n",
    "# as you can see it's quite hard to decipher\n",
    "\n",
    "source.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(source.content) # type bytes in Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert source.content to a beautifulsoup object \n",
    "# beautifulsoup can parse (extract specific information) HTML code\n",
    "\n",
    "soup = bs.BeautifulSoup(source.content, features='html.parser') \n",
    "# we pass in the source content\n",
    "# features specifies what type of code we are parsing, \n",
    "# here 'html.parser' specifies that we want beautiful soup to parse HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup) # looks a lot nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we printed the HTML code of the website, decoded as a beautiful soup object.\n",
    "\n",
    "### HTML tags\n",
    "`<xxx> </xxx>`: are all the HTML tags, that specifies certain sections, stylings etc of the website, for more info: \n",
    "https://www.w3schools.com/tags/ref_byfunc.asp\n",
    "\n",
    "Full list of HTML tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML DOM (Document Object Model) Tree\n",
    "\n",
    "The HTML DOM Tree is a logical tree that contains all the objects in a webpage.\n",
    "\n",
    "Any dynamic execution (Javascript, SVG etc) interacts with the DOM tree.\n",
    "\n",
    "Since HTML content has a hierarchy, a Tree structure appropriately models the relationships between different HTML elements.\n",
    "\n",
    "For more, see the [Mozilla Docs](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model). There's a great intro to the DOM [here](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we want to extract content that is shown on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the <body> tag of the website is where all the main content is\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.title) # Title of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find('title')) # same as .title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to extract specific text\n",
    "print(soup.find('p')) # will only return first <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find('p').text) # extracts the string within the <p> tag, strips it of tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to extract all <p> tags\n",
    "print(soup.find_all('p')) # returns list of all <p> tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# HTML Attributes\n",
    "\n",
    "## `class` and `id`:\n",
    "\n",
    "class and id attributes of HTML tags, they are used as hooks to give unique styling to certain elements and an id for sections / parts of the page.\n",
    "\n",
    "- **id:** is a unique tag for a specific element (this often does not change)\n",
    "- **class:** specifies a class of objects. Several elements in the HTML code can have the same class.\n",
    "\n",
    "We can use these attributes of an HTML tag to select elements in BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also search for classes within all tags, using class_\n",
    "# note _ is used to distinguish with Python's builtin class function\n",
    "\n",
    "print(soup.find(class_='header')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also find tags with a speific id\n",
    "\n",
    "print(soup.find(id='second'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all(class_='regular_list')) # find all returns list, \n",
    "# even if there is only one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'): # print all text paragraphs on the webpage\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract links / urls\n",
    "# Links in html is usually coded as <a href=\"url\">\n",
    "# where the link is url\n",
    "\n",
    "print(soup.a)\n",
    "print(type(soup.a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.get('href') \n",
    "# to get the link from href attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to list links and their text info\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for l in links:\n",
    "    print(\"Info about {}: \".format(l.text), \\\n",
    "          l.get('href')) \n",
    "# then we have extracted the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSVAs1SnUg1E"
   },
   "source": [
    "# Scraping Caveats: How to be nice and not make enemies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv2ExOfJUg1E"
   },
   "source": [
    "- Webscraping is not always a welcome activity. \n",
    "    As a founder and/or engineer, you don't want to wake up in the middle of the night because your website is down due to scraping!\n",
    "    \n",
    "- When webscraping a website, be mindful and nice and make sure you are not inadvertently sending too many requests to the website, which can lead to a potential problem at the website.\n",
    "\n",
    "- A pretty common reason for websites going temporarily offline is because they get scraped way too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3wlRA0gUg1E"
   },
   "source": [
    "## What do we lookout for when scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxL9Wp-mUg1F"
   },
   "source": [
    "### Cached content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvYWEBSlUg1F"
   },
   "source": [
    "- Most website content is usually `cached`. This means the webservers are not serving the content directly, they are cached at a nearby caching server (usually called Point-of-Presence or POPs). Web requests usually hit these servers that are able to serve cached content at a higher frequency.\n",
    "\n",
    "    - You might have heard of services such as AWS Cloudfront or Cloudflare allowing web content to be cached.\n",
    "\n",
    "\n",
    "- That said, some websites might not do this! One way to check is to see the response headers. Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "IFUjGnrpUg1G",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2b32a8b4-2970-448e-a96c-f91988094264"
   },
   "outputs": [],
   "source": [
    "requests.get('https://wikipedia.org').headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxXBekcKUg1I"
   },
   "source": [
    "The `Cache-Control` header having a non-zero value means the web content is cached.\n",
    "\n",
    "The `X-Cache-Status` equaling `hit-front` or `hit` or similar means you hit a cached content, and the backend server did not directly service request.\n",
    "\n",
    "Yay! This means we can scrape without worrying about taking down Wikipedia.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other useful scraping tips\n",
    "\n",
    "### robots.txt\n",
    "\n",
    "Always check if a webiste has a `robots.txt` document specifying what parts of the site that you're allowed to scrape (however, the website cannot prevent requests from getting its content, but I'd recommend you all to be nice). It may also contain information about the scraping frequency allowed etc.\n",
    "\n",
    "E.g. \n",
    "- https://www.wikipedia.com/robots.txt\n",
    "- https://www.imdb.com/robots.txt\n",
    "- https://www.nytimes.com/robots.txt\n",
    "\n",
    "### user-agent\n",
    "\n",
    "When you're sending a request to a webpage (no matter if it comes from your computer, iphone, or Python's request package), then you also include a user-agent. This let's the webserver know how to render the contents for you. You can also send user-agent information via a request (to specify who you are for example, or to disguise that you're an automated scraper).\n",
    "\n",
    "Find your machine's / browser's true user agent here: https://www.whoishostingthis.com/tools/user-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-agent example\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:69.0) Gecko/20100101 Firefox/69.0',\n",
    "    'From': 'data-x@gmail.com' \n",
    "}\n",
    "\n",
    "response = requests.get('http://alex.fo/other/data-x', headers=headers)\n",
    "print(response)\n",
    "print(response.headers) # the response will also have some meta informaiton about the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "\n",
    "# Data-X website Scraping\n",
    "### Now let us scrape the current Syllabus Schedule from the Data-X website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get('https://web.archive.org/web/20181121185625/https://data-x.blog/')\n",
    "# get the source content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs.BeautifulSoup(source.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()) \n",
    "# .prettify() method makes the HTML code more readable\n",
    "\n",
    "# as you can see this code is more difficult \n",
    "# to read then the simple example above\n",
    "# mostly because this is a real Wordpress website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the Title of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.find('title').text) \n",
    "# check that we are at the correct website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all paragraphs of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the navigation bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigation_bar = soup.find('nav')\n",
    "print(navigation_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the linked subpages in the navigation bar\n",
    "nav_bar = navigation_bar.text\n",
    "print(nav_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the Syllabus of its content\n",
    "(maybe to use in an App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigation_bar.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to find the Syllabus, \n",
    "# however we are at the root web page, not displaying the Syllabus\n",
    "\n",
    "# Get all links from navigation bar at the data-x home webpage\n",
    "for url in navigation_bar.find_all('a'): \n",
    "    link = url.get('href')\n",
    "    if 'data-x.blog' in link: # check link to a subpage\n",
    "        print(link) \n",
    "        if 'syllabus' in link:\n",
    "            syllabus_url = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllabus is located at https://data-x.blog/syllabus/\n",
    "print(syllabus_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open new connection to the Syllabus url. Replace soup object.\n",
    "\n",
    "source = requests.get(syllabus_url).content\n",
    "soup = bs.BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "print(soup.body.prettify()) \n",
    "# we can see that the Syllabus is built up of <td>, <tr> and <table> tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the course schedule table from the syllabus:  \n",
    "Usually organized data in HTML format on a website is stored in tables under `<table>, <tr>,` and `<td>` tags. Here we want to extract the information in the Data-X syllabus.\n",
    "\n",
    "**NOTE:**  To identify element, class or id  name of the object of your interest on a web page, you can go to the link address in your browser, under 'more tools' option click __'developer tools'__. This opens  the 'Document Object Model' of the webpage. Hover on the element of your interest on the webpage to check its location. This will help you in deciding which parts of 'soup content' you want to parse. More info at: https://developer.chrome.com/devtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can see that course schedule is in <table><table/> elements\n",
    "# We can also get the table\n",
    "full_table = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A new row in an HTML table starts with <tr> tag\n",
    "# A new column entry is defined by <td> tag\n",
    "table_result = list()\n",
    "for table in full_table:\n",
    "    for row in table.find_all('tr'):\n",
    "        row_cells = row.find_all('td') # find all table data\n",
    "        row_entries = [cell.text for cell in row_cells]\n",
    "        print(row_entries) \n",
    "        table_result.append(row_entries)\n",
    "        # get all the table data into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also read it in to a Pandas DataFrame\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "df = pd.DataFrame(table_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas can also grab tables from a website automatically\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dfs = pd.read_html(syllabus_url) \n",
    "# returns a list of all tables at url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dfs)) #list of tables\n",
    "print(len(dfs)) # we only have one table\n",
    "print(type(dfs[0])) # stored as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs[2:],ignore_index=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks so-so, however striped from break line characters etc.\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it nicer\n",
    "\n",
    "# Assign column names\n",
    "df.columns=  ['Part','Detailed Description']\n",
    "\n",
    "# Assing week number\n",
    "weeks = list()\n",
    "i=0\n",
    "for k in range(df.shape[0]):\n",
    "    if 'Topic' in df.iloc[k,0]:\n",
    "        i=i+1\n",
    "    weeks.append('Lecture{}'.format(i))\n",
    "df['Week'] = weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Week and Part as Multiindex\n",
    "df = df.set_index(['Week','Part'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(12).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep a current list IMDB top 250 vs MetaScore\n",
    "\n",
    "Let's say that we want to build an app that can display the most popular movies at the IMDB website.\n",
    "\n",
    "We got to the URL that lists the top 250 movies according to the reviews: http://www.imdb.com/chart/top\n",
    "\n",
    "We see that the entries are stored in a table format, so we try pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for english version\n",
    "r = requests.get(url, headers = {\"Accept-Language\": \"en-US\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imdb = pd.read_html(r,attrs={'class':'chart full-width'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.drop(df_imdb.columns[[0,3,4]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all URLs to find meta score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the HTML content of the top 200 list\n",
    "imdb_html = requests.get('http://www.imdb.com/chart/top', headers = {\"Accept-Language\": \"en-US\"}).content\n",
    "soup = bs.BeautifulSoup(imdb_html, features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all links to other imdb movies\n",
    "links = soup.find('table').find_all('a')\n",
    "urls = ['http://www.imdb.com'+l.get('href') for l in links]\n",
    "urls = urls[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "meta_scores = np.zeros(250, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://www.imdb.com/title/tt0068646/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs.BeautifulSoup(res.content,features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect meta scores for top 6 movies\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0',\n",
    "    'From': 'data-x@gmail.com', \"Accept-Language\": \"en-US\" \n",
    "}\n",
    "\n",
    "for idx,url in enumerate(urls):\n",
    "    print('Getting metscore for movie {} / 250'.format(idx+1))\n",
    "    film = requests.get(url, headers=headers, timeout=10)\n",
    "    print(film)\n",
    "    print(url)\n",
    "    soup = bs.BeautifulSoup(film.content, features='html.parser')\n",
    "    try:\n",
    "        info = soup.find(class_='metacriticScore score_favorable titleReviewBarSubItem').find('span').text\n",
    "    except:\n",
    "        info = soup.find(class_='score-meta').text\n",
    "    meta_scores[idx] = int(info)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb['meta_scores'] = meta_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "# Scrape images and other files\n",
    "\n",
    "Let's see how we can automatically find and download files linked at any website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see there are many images at https://data-x.blog/projects/\n",
    "# say that we want to download them\n",
    "# Images are displayed with the <img> tag in HTML\n",
    "\n",
    "# open connection and create new soup\n",
    "\n",
    "raw = requests.get('https://web.archive.org/web/20181121185625/https://data-x.blog/').content\n",
    "soup = bs.BeautifulSoup(raw,features='html.parser')\n",
    "\n",
    "print(soup.find('img')) \n",
    "# as we can see below the image urls \n",
    "# are stored in the src attribute inside the img tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse all url to the images\n",
    "img_urls = list()\n",
    "for img in soup.find_all('img'): \n",
    "    img_url = img.get('src') \n",
    "    if '.jpeg' in img_url or '.jpg' in img_url or 'png' in img_url:\n",
    "        print(img_url)\n",
    "        img_urls.append(img_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download and save files with Python we can use \n",
    "# the shutil library which is a file operations library\n",
    "'''\n",
    "The shutil module offers a number of high-level operations on files and \n",
    "collections of files. In particular, functions are provided which support \n",
    "file copying and removal.\n",
    "'''\n",
    "\n",
    "import shutil\n",
    "\n",
    "for idx, img_url in enumerate(img_urls): \n",
    "    #enumarte to create a file integer name for every image\n",
    "    \n",
    "    # make a request to the image URL\n",
    "    img_source = requests.get(img_url, stream=True) \n",
    "    # we set stream = True to download/ \n",
    "    # stream the content of the data\n",
    "    \n",
    "    with open('img'+str(idx)+'.jpg', 'wb') as file: \n",
    "        # open file connection, create file and write to it\n",
    "        shutil.copyfileobj(img_source.raw, file) \n",
    "        # save the raw file object\n",
    "\n",
    "    del img_source # to remove the file from memory\n",
    "    if idx>4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function to download files of any type from a website\n",
    "\n",
    "Below is a function that takes in a website and a specific file type to download X of them from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To interact with operating system and format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" \n",
    "    in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or \n",
    "    a for file links)\n",
    "    \n",
    "    source_tag = the source tag for the file url \n",
    "    (usually src for images or href for files)\n",
    "    \n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    \n",
    "    max = integer (max number of files to scrape, \n",
    "    if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' \n",
    "    # for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "    print('Loading content from the url...')\n",
    "    source = requests.get(url).content\n",
    "    print('Creating content soup...')\n",
    "    soup = bs.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    i=0\n",
    "    print('Finding tag:%s...'%html_tag)\n",
    "    for n, link in enumerate(soup.find_all(html_tag)):\n",
    "        file_url=link.get(source_tag)\n",
    "        print ('\\n',n+1,'. File url',file_url)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "            print('It is a valid url..')\n",
    "            \n",
    "            \n",
    "            if file_type in file_url: #only check for specific file types\n",
    "                \n",
    "                print('%s FILE TYPE FOUND IN THE URL...'%file_type)\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "             \n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and \n",
    "                    # write to it\n",
    "                    \n",
    "                    shutil.copyfileobj(file_source.raw, file) \n",
    "                    # save the raw file object\n",
    "                    \n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('%s file type NOT found in url:'%file_type)\n",
    "                print('EXCLUDED:',file_url) \n",
    "                # urls not downloaded from\n",
    "                \n",
    "        if i == max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape funny cat pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('https://funcatpictures.com/') \n",
    "# scrape cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ./files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape pdf's from Data-X site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('https://web.archive.org/web/20181121185625/https://data-x.blog/resources',\n",
    "                html_tag='a',source_tag='href',file_type='.pdf', \\\n",
    "                max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape real data CSV files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('http://www-eio.upc.edu/~pau/cms/rdata/datasets.html',\n",
    "                html_tag='a', # R data sets\n",
    "                source_tag='href', file_type='.csv',max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended tip: IP rotation\n",
    "\n",
    "The website might get suspicious if a lot of requests are coming from the same IP address. If you use a shared proxy, VPN or TOR that can help you get around that problem\n",
    "\n",
    "For example:\n",
    "\n",
    "```pyton\n",
    "proxies = {'http' : 'http://10.10.0.0:0000',  \n",
    "          'https': 'http://120.10.0.0:0000'}\n",
    "response = requests.get('https://whateverwebsite.com', proxies=proxies, timeout=5)\n",
    "\n",
    "```\n",
    "\n",
    "Also note the `timeout` argument, this specifies that the request should not be carried out indefinitely (prevents the webserver from detecting scraping activity).\n",
    " \n",
    "\n",
    "By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnMeFM0XUg3S"
   },
   "source": [
    "## Again, be NICE. Don't use IP rotation unless you know the content is cached and/or you know the website can handle your load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pgxog_7UUg3S"
   },
   "source": [
    "## Websites often rely on IP blocking, rate limiting and other techniques to either:\n",
    "\n",
    "a) Dissuade web scraping\n",
    "\n",
    "b) Block large web scraping requests that turn to Denial of Service (DOS)\n",
    "\n",
    "c) Block scraping on restricted parts of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9rho3HLUg3S"
   },
   "source": [
    "## IP Blocking\n",
    "- The IP address of your machine(s) is temporarily blocked. (If you are using a VPN, often times this means the *VPNs* IP is blocked, which can inconvenience others using the VPN. Be careful.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z798_m49Ug3T"
   },
   "source": [
    "## Rate Limiting\n",
    "- You might get a 4xx (429) http error if you scrape websites at a higher rate than they can handle. Rate limiting is often multi-tiered, i.e you can be limited per second (concurrent requests), per minute or per hour. Rate limits are sometimes temporary but can be permanent (IP Block) if you routinely hit rate limits.\n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/8abAbOrQ9rvLG/giphy.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgdY-QZ4Ug3T"
   },
   "source": [
    "## Honeypot Servers\n",
    "- If you are not nice and continue scraping websites at high rates, websites can route your request to special \"Honeypot\" Servers, which essentially redirect your requests to servers that are designed to waste your CPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "# Scrape Bloomberg sitemap (XML) for current political news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps\n",
    "\n",
    "# Before scraping a website look at robots.txt file\n",
    "bs.BeautifulSoup(requests.get('https://www.bloomberg.com/robots.txt').content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').content\n",
    "soup = bs.BeautifulSoup(source,'xml') # Note parser 'xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find political news headlines\n",
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    #print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "# Web crawl\n",
    "\n",
    "Web crawling is almost like webscraping, but instead you crawl a specific website (and often its subsites) and extract meta information. It can be seen as simple, recursive scraping. This can be used for web indexing (in order to build a web search engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawl Twitter account\n",
    "**Authors:** Kunal Desai & Alexander Fred Ojala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to maintain the urls and the number of times they appear\n",
    "\n",
    "url_dict = dict()\n",
    "\n",
    "def add_to_dict(url_d, key):\n",
    "    if key in url_d:\n",
    "        url_d[key] = url_d[key] + 1\n",
    "    else:\n",
    "        url_d[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and \"https://\" in link['href']:\n",
    "#             print(link['href'])\n",
    "            add_to_dict(url_dict, link['href'])\n",
    "            get_urls(link['href'], depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls_iterative(url, depth):\n",
    "    urls = [url]\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and \"https://\" in link['href']:\n",
    "                add_to_dict(url_dict, link['href'])\n",
    "                urls.append(link['href'])\n",
    "        if len(urls) > depth:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls(\"https://twitter.com/GolfWorld\", 2)\n",
    "for key in url_dict:\n",
    "    print(str(key) + \"  ----   \" + str(url_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec8'></a>\n",
    "# SEO: Visualize sitemap and categories in a website\n",
    "\n",
    "**Source:** https://www.ayima.com/guides/how-to-visualize-an-xml-sitemap-using-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XML sitemap with categories!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.sportchek.ca/sitemap.xml'\n",
    "url = 'https://www.bloomberg.com/feeds/bpol/sitemap_index.xml'\n",
    "page = requests.get(url)\n",
    "print('Loaded page with: %s' % page)\n",
    "\n",
    "sitemap_index = BeautifulSoup(page.content, 'html.parser')\n",
    "print('Created %s object' % type(sitemap_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [element.text for element in sitemap_index.findAll('loc')]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    ''' Open an XML sitemap and find content wrapped in loc tags. '''\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = [element.text for element in soup.findAll('loc')]\n",
    "\n",
    "    return links\n",
    "\n",
    "sitemap_urls = []\n",
    "for url in urls:\n",
    "    links = extract_links(url)\n",
    "    sitemap_urls += links\n",
    "\n",
    "print('Found {:,} URLs in the sitemap'.format(len(sitemap_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sitemap_urls.dat', 'w') as f:\n",
    "    for url in sitemap_urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Categorize a list of URLs by site path.\n",
    "The file containing the URLs should exist in the working directory and be\n",
    "named sitemap_urls.dat. It should contain one URL per line.\n",
    "Categorization depth can be specified by executing a call like this in the\n",
    "terminal (where we set the granularity depth level to 5):\n",
    "    python categorize_urls.py --depth 5\n",
    "The same result can be achieved by setting the categorization_depth variable\n",
    "manually at the head of this file and running the script with:\n",
    "    python categorize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "categorization_depth=3\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "\n",
    "def peel_layers(urls, layers=3):\n",
    "    ''' Builds a dataframe containing all unique page identifiers up\n",
    "    to a specified depth and counts the number of sub-pages for each.\n",
    "    Prints results to a CSV file.\n",
    "    urls : list\n",
    "        List of page URLs.\n",
    "    layers : int\n",
    "        Depth of automated URL search. Large values for this parameter\n",
    "        may cause long runtimes depending on the number of URLs.\n",
    "    '''\n",
    "\n",
    "    # Store results in a dataframe\n",
    "    sitemap_layers = pd.DataFrame()\n",
    "\n",
    "    # Get base levels\n",
    "    bases = pd.Series([url.split('//')[-1].split('/')[0] for url in urls])\n",
    "    sitemap_layers[0] = bases\n",
    "\n",
    "    # Get specified number of layers\n",
    "    for layer in range(1, layers+1):\n",
    "\n",
    "        page_layer = []\n",
    "        for url, base in zip(urls, bases):\n",
    "            try:\n",
    "                page_layer.append(url.split(base)[-1].split('/')[layer])\n",
    "            except:\n",
    "                # There is nothing that deep!\n",
    "                page_layer.append('')\n",
    "\n",
    "        sitemap_layers[layer] = page_layer\n",
    "\n",
    "    # Count and drop duplicate rows + sort\n",
    "    sitemap_layers = sitemap_layers.groupby(list(range(0, layers+1)))[0].count()\\\n",
    "                     .rename('counts').reset_index()\\\n",
    "                     .sort_values('counts', ascending=False)\\\n",
    "                     .sort_values(list(range(0, layers)), ascending=True)\\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "    # Convert column names to string types and export\n",
    "    sitemap_layers.columns = [str(col) for col in sitemap_layers.columns]\n",
    "    sitemap_layers.to_csv('sitemap_layers.csv', index=False)\n",
    "\n",
    "    # Return the dataframe\n",
    "    return sitemap_layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sitemap_urls = open('sitemap_urls.dat', 'r').read().splitlines()\n",
    "print('Loaded {:,} URLs'.format(len(sitemap_urls)))\n",
    "\n",
    "print('Categorizing up to a depth of %d' % categorization_depth)\n",
    "sitemap_layers = peel_layers(urls=sitemap_urls,\n",
    "                             layers=categorization_depth)\n",
    "print('Printed {:,} rows of data to sitemap_layers.csv'.format(len(sitemap_layers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize a list of URLs by site path.\n",
    "This script reads in the sitemap_layers.csv file created by the\n",
    "categorize_urls.py script and builds a graph visualization using Graphviz.\n",
    "Graph depth can be specified by executing a call like this in the\n",
    "terminal:\n",
    "    python visualize_urls.py --depth 4 --limit 10 --title \"My Sitemap\" --style \"dark\" --size \"40\"\n",
    "The same result can be achieved by setting the variables manually at the head\n",
    "of this file and running the script with:\n",
    "    python visualize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# Set global variables\n",
    "\n",
    "graph_depth = 3  # Number of layers deep to plot categorization\n",
    "limit = 3       # Maximum number of nodes for a branch\n",
    "title = ''       # Graph title\n",
    "style = 'light'  # Graph style, can be \"light\" or \"dark\"\n",
    "size = '8,5'     # Size of rendered PDF graph\n",
    "\n",
    "\n",
    "# Import external library dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "def make_sitemap_graph(df, layers=3, limit=50, size='8,5'):\n",
    "    ''' Make a sitemap graph up to a specified layer depth.\n",
    "    sitemap_layers : DataFrame\n",
    "        The dataframe created by the peel_layers function\n",
    "        containing sitemap information.\n",
    "    layers : int\n",
    "        Maximum depth to plot.\n",
    "    limit : int\n",
    "        The maximum number node edge connections. Good to set this\n",
    "        low for visualizing deep into site maps.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Check to make sure we are not trying to plot too many layers\n",
    "    if layers > len(df) - 1:\n",
    "        layers = len(df)-1\n",
    "        print('There are only %d layers available to plot, setting layers=%d'\n",
    "              % (layers, layers))\n",
    "\n",
    "\n",
    "    # Initialize graph\n",
    "    f = graphviz.Digraph('sitemap', filename='sitemap_graph_%d_layer' % layers)\n",
    "    f.body.extend(['rankdir=LR', 'size=\"%s\"' % size])\n",
    "\n",
    "\n",
    "    def add_branch(f, names, vals, limit, connect_to=''):\n",
    "        ''' Adds a set of nodes and edges to nodes on the previous layer. '''\n",
    "\n",
    "        # Get the currently existing node names\n",
    "        node_names = [item.split('\"')[1] for item in f.body if 'label' in item]\n",
    "\n",
    "        # Only add a new branch it it will connect to a previously created node\n",
    "        if connect_to:\n",
    "            if connect_to in node_names:\n",
    "                for name, val in list(zip(names, vals))[:limit]:\n",
    "                    f.node(name='%s-%s' % (connect_to, name), label=name)\n",
    "                    f.edge(connect_to, '%s-%s' % (connect_to, name), label='{:,}'.format(val))\n",
    "\n",
    "\n",
    "    f.attr('node', shape='rectangle') # Plot nodes as rectangles\n",
    "\n",
    "    # Add the first layer of nodes\n",
    "    for name, counts in df.groupby(['0'])['counts'].sum().reset_index()\\\n",
    "                          .sort_values(['counts'], ascending=False).values:\n",
    "        f.node(name=name, label='{} ({:,})'.format(name, counts))\n",
    "\n",
    "    if layers == 0:\n",
    "        return f\n",
    "\n",
    "    f.attr('node', shape='oval') # Plot nodes as ovals\n",
    "    f.graph_attr.update()\n",
    "\n",
    "    # Loop over each layer adding nodes and edges to prior nodes\n",
    "    for i in range(1, layers+1):\n",
    "        cols = [str(i_) for i_ in range(i)]\n",
    "        nodes = df[cols].drop_duplicates().values\n",
    "        for j, k in enumerate(nodes):\n",
    "\n",
    "            # Compute the mask to select correct data\n",
    "            mask = True\n",
    "            for j_, ki in enumerate(k):\n",
    "                mask &= df[str(j_)] == ki\n",
    "\n",
    "            # Select the data then count branch size, sort, and truncate\n",
    "            data = df[mask].groupby([str(i)])['counts'].sum()\\\n",
    "                    .reset_index().sort_values(['counts'], ascending=False)\n",
    "\n",
    "            # Add to the graph\n",
    "            add_branch(f,\n",
    "                       names=data[str(i)].values,\n",
    "                       vals=data['counts'].values,\n",
    "                       limit=limit,\n",
    "                       connect_to='-'.join(['%s']*i) % tuple(k))\n",
    "\n",
    "            print(('Built graph up to node %d / %d in layer %d' % (j, len(nodes), i))\\\n",
    "                    .ljust(50), end='\\r')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_style(f, style, title=''):\n",
    "    ''' Apply the style and add a title if desired. More styling options are\n",
    "    documented here: http://www.graphviz.org/doc/info/attrs.html#d:style\n",
    "    f : graphviz.dot.Digraph\n",
    "        The graph object as created by graphviz.\n",
    "    style : str\n",
    "        Available styles: 'light', 'dark'\n",
    "    title : str\n",
    "        Optional title placed at the bottom of the graph.\n",
    "    '''\n",
    "\n",
    "    dark_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'bgcolor': '#3a3a3a',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'white',\n",
    "            'fillcolor': 'black',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'white',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'white',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    light_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'black',\n",
    "            'fillcolor': '#dbdddd',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'black',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'black',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if style == 'light':\n",
    "        apply_style = light_style\n",
    "\n",
    "    elif style == 'dark':\n",
    "        apply_style = dark_style\n",
    "\n",
    "    f.graph_attr = apply_style['graph']\n",
    "    f.node_attr = apply_style['nodes']\n",
    "    f.edge_attr = apply_style['edges']\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read in categorized data\n",
    "sitemap_layers = pd.read_csv('sitemap_layers.csv', dtype=str)\n",
    "# Convert numerical column to integer\n",
    "sitemap_layers.counts = sitemap_layers.counts.apply(int)\n",
    "print('Loaded {:,} rows of categorized data from sitemap_layers.csv'\\\n",
    "        .format(len(sitemap_layers)))\n",
    "\n",
    "print('Building %d layer deep sitemap graph' % graph_depth)\n",
    "f = make_sitemap_graph(sitemap_layers, layers=graph_depth,\n",
    "                       limit=limit, size=size)\n",
    "f = apply_style(f, style=style, title=title)\n",
    "\n",
    "f.render(cleanup=True)\n",
    "print('Exported graph to sitemap_graph_%d_layer.pdf' % graph_depth)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
