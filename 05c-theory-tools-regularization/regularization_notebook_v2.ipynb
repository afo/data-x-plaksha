{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-X\n",
    "## Tutorial on Regularization, Ridge, and Lasso Regression \n",
    "Reference:  \n",
    "* AARSHAY JAIN - A Complete Tutorial on Ridge and Lasso Regression in Python\n",
    "* https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Ridge and Lasso Regression are regularization techniques generally used for creating simple models (Occam's Razor) in presence of a large number of features. The main goal is to combat tendency of models to overfit and balance the statistical / computational tradeoffs that are ubiquitous in the high-dimensional statistics era. \n",
    "\n",
    "* **Ridge Regression**\n",
    "    - L2-regularization \n",
    "    - Obj = Loss Function + $\\alpha\\|\\beta\\|_2$\n",
    "* **Lasso Regression**\n",
    "    - L1-regularization\n",
    "    - Obj = Loss Function + $\\alpha\\|\\beta\\|_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14, 10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why penalize the magnitude of coefficients?\n",
    "\n",
    "Consider the following simulation of a sine curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x_full = np.array([i*np.pi/180 for i in range(60,300,2)])\n",
    "\n",
    "np.random.seed(10)  #Setting seed for reproducability\n",
    "y_full = np.sin(x_full) + np.random.normal(0,0.15,len(x_full)) # Adding Noise\n",
    "\n",
    "# split into train and test sets\n",
    "x, x_test, y, y_test = train_test_split(x_full,y_full,test_size=0.25)\n",
    "\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y']) # train data\n",
    "data_test = pd.DataFrame(np.column_stack([x_test,y_test]),columns=['x','y']) # test data\n",
    "\n",
    "plt.plot(data['x'],data['y'],'.',ms=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list w pointers to train and test DF\n",
    "combine = [data, data_test]\n",
    "\n",
    "for df in combine:\n",
    "    # so that plotting makes sense\n",
    "    # x values need to be sorted\n",
    "    df.sort_values('x',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Thoughts: Polynomial Regression \n",
    "Features: $X^1, X^2, \\dots, X^{15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial columns\n",
    "for df in combine:\n",
    "\n",
    "    for i in range(2,16):  #power of 1 is already there\n",
    "        colname = 'x^%d'%i      #new var will be x^power\n",
    "        df[colname] = df['x']**i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider building 15 models as follows:\n",
    "* Model 1: Features - $X^1$\n",
    "* Model 2: Features - $X, X^2$\n",
    "* $\\dots$\n",
    "* Model 15: Features - $X, X^2, X^{15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEval:\n",
    "    def __init__(self, data, data_test, models_to_plot, model_name='Model', alphas = None):\n",
    "        self.data = data\n",
    "        self.data_test = data_test\n",
    "        self.models_to_plot = models_to_plot\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        #Initialize a dataframe to store the results:\n",
    "        col = ['rss_train','rss_test','intercept'] + ['coef_x^%d'%i for i in range(1,16)]\n",
    "        if alphas:\n",
    "            ind = ['alpha_%.2g'%alphas[i] for i in range(len(alphas))]\n",
    "        else:\n",
    "            ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "        \n",
    "        self.results = pd.DataFrame(index=ind, columns=col)\n",
    "        self.results.index.name = model_name\n",
    "        \n",
    "        self.count = 0\n",
    "        \n",
    "    def mod_name(self,name):\n",
    "        self.model_name = name\n",
    "        self.results.index.name = name\n",
    "            \n",
    "    def fit_predict_plot(self,model, power=15, alpha = None):\n",
    "        \n",
    "        predictors=['x']\n",
    "        if power>=2:\n",
    "            predictors.extend(['x^%d'%i for i in range(2,power+1)])\n",
    "        \n",
    "        model.fit(self.data[predictors], self.data['y'])\n",
    "        ypred = model.predict(self.data[predictors])\n",
    "        \n",
    "        rss = sum((ypred-data['y'])**2)\n",
    "        self.metrics = [rss]\n",
    "    \n",
    "        y_pred_test = model.predict(self.data_test[predictors])\n",
    "        rss_test = sum((y_pred_test-self.data_test['y'])**2)\n",
    "        self.metrics.extend([rss_test])\n",
    "    \n",
    "        self.metrics.extend([model.intercept_])\n",
    "        self.metrics.extend(model.coef_)\n",
    "\n",
    "        \n",
    "        self.results.iloc[self.count,0:power+3] = self.metrics\n",
    "        self.count = self.count+1\n",
    "        \n",
    "        if power in self.models_to_plot or alpha in self.models_to_plot:\n",
    "            if alpha:\n",
    "                plt.plot(self.models_to_plot[alpha])\n",
    "                plt.title('Plot for model {} alpha: {}'.format(self.model_name,str(alpha)))\n",
    "            else:\n",
    "                plt.plot(self.models_to_plot[power])\n",
    "                plt.title('Plot for model {} power: {}'.format(self.model_name,str(power)))\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.plot(self.data['x'],ypred)\n",
    "            plt.plot(self.data['x'],self.data['y'],'.',ms=10)\n",
    "            plt.ylim([-1.5,1.5])\n",
    "            plt.xlim([1,5.5])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "modeval = ModelEval(data, data_test, models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through all powers and assimilate results\n",
    "from sklearn.linear_model import LinearRegression\n",
    "modeval.mod_name('Linear Regression')\n",
    "mod = LinearRegression(normalize=True)\n",
    "\n",
    "for i in range(1,16):\n",
    "    modeval.fit_predict_plot(model=mod, power=i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly aligns with our initial understanding. As the model complexity increases, the models tends to fit even smaller deviations in the training data set. We start to fit the gaussian noise in the data (patterns that is not there)\n",
    "\n",
    "However, as we add more features, we expect ourselves to overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "modeval.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly evident that the size of coefficients increase exponentially with increase in model complexity.\n",
    "\n",
    "What does a large coefficient signify? \n",
    "\n",
    "It means that we’re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome. When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Note the ‘Ridge’ function used here. It takes ‘alpha’ as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. Note that each of these 10 models will contain all the 15 variables and only the value of alpha would differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "mods_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "\n",
    "modeval = ModelEval(data, data_test, models_to_plot = mods_to_plot, alphas=alpha_ridge)\n",
    "modeval.mod_name('Ridge Regression')\n",
    "\n",
    "for i in range(10):\n",
    "    mod = Ridge(alpha = alpha_ridge[i], normalize=True)\n",
    "    modeval.fit_predict_plot(model=mod, alpha = mod.alpha) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (eg. alpha = 5). Thus alpha should be chosen wisely. A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.\n",
    "\n",
    "**USE CROSS VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "modeval.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "This straight away gives us the following inferences:\n",
    "\n",
    "1. The RSS_train increases with increase in alpha, while rss_test is optimized when alpha is around 0.0001 (note edge cases not perfect)\n",
    "2. An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients. How? Compare the coefficients in the first row of this table to the last row of simple linear regression table.\n",
    "3. High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
    "4. Though the coefficients are **very very small, they are NOT zero**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many of the coefficents are zero (in the full coefficient matrix)\n",
    "\n",
    "modeval.results.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that none of the coefficients are 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression \n",
    "#### (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "mods_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "modeval = ModelEval(data, data_test, models_to_plot=mods_to_plot, alphas=alpha_lasso)\n",
    "modeval.mod_name('Lasso Regression')\n",
    "\n",
    "for i in range(10):\n",
    "    mod = Lasso(alpha = alpha_lasso[i], normalize=True, max_iter=1e5)\n",
    "    modeval.fit_predict_plot(model=mod, alpha = mod.alpha) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "modeval.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have high sparsity whenever a model coefficicent is zero.\n",
    "\n",
    "Seems to be best around alpha 10^-5 or 10^-4.\n",
    "\n",
    "### Inference:\n",
    "\n",
    "Apart from the expected inference of higher RSS for higher alphas, we can see the following:\n",
    "\n",
    "1. For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables).\n",
    "2. For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression\n",
    "3. Many of the coefficients are zero even for very small values of alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeval.results.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as $\\alpha$ increases, the number of coefficients being set to zero increases from 0 to 15. Implicitly, Lasso conducts feature selection as well!\n",
    "\n",
    "- Lasso works the best with sparsity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Key Differences:\n",
    "    - Ridge: Typically includes all (or none) of the features in the model. \n",
    "        - Main advantage: Coefficient Shrinkage & Reducing Model Complexity\n",
    "        - It generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.\n",
    "        - Prevents overfitting but does not reduce computational challenges\n",
    "        - simple gradient descent is capable of the optimization\n",
    "    - Lasso: \n",
    "        - Main advantage: Reduces model complexity & performs coefficient shrinking as well as feature selection\n",
    "        - Provides sparse solutions\n",
    "        - requires subgradient optimization in gradient descent\n",
    "\n",
    "* You can combine Lasso and Ridge Regression via Elastic Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration: Regularizing with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge with stochastic gradient descent\n",
    "iterations = [1,5,50,500,5e3,3e4]\n",
    "plt.subplots(2,3,figsize=(20,14))\n",
    "\n",
    "predictors=['x']\n",
    "predictors.extend(['x^%d'%i for i in range(2,16)])\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    its = iterations[i]\n",
    "    \n",
    "    model = Ridge(alpha=0, normalize=True,max_iter=its,solver='sag',tol=1e-9)\n",
    "    model.fit(data[predictors], data['y'])\n",
    "    y_pred = model.predict(data[predictors])\n",
    "    plt.plot(data['x'],data['y'],'.',ms=16)\n",
    "    plt.plot(data['x'],y_pred)\n",
    "    plt.title('Max Iterations {}'.format(its))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
