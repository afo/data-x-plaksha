{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# __Titanic Survivor Analysis__\n",
    "\n",
    "#### Author: Several publc Kaggle Kernels\n",
    "#### Edits: Alexander Fred Ojala, Kevin Li, and Elias Castro Hernandez\n",
    "\n",
    "**Sources:** \n",
    "* **Training + explanations**: https://www.kaggle.com/c/titanic\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://static01.nyt.com/images/2018/10/24/travel/24xp-titanic-tear/24xp-titanic-tear-superJumbo.jpg' width=400px></center>\n",
    "\n",
    "# Understanding the connections between passanger information and survival rate\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "<br>\n",
    "The data conists of passanger information for the maiden, and tragic, voyage of the Titanic ocean liner. The set is comprised of 1309 unique entries, each associated with information realated to a passanger. The set is broken is broken down to 891/418 (testing/training) split for the purposes of modeling\n",
    "\n",
    "**Our task is to train a machine learning model on the training set in order to predict if the passengers in the test set survived or not.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "> #### [Part 0: Install Additional Required Software](#Part-0:-Install-Additional-Required-Software)\n",
    "\n",
    "> #### [Part 1: Import Packages + Libraries + Dependencies](#Part-1:-Import-Packages-+-Libraries-+-Dependencies)\n",
    "\n",
    "> #### [Part 2: Preprocessing and Exploratory Data Anlysis](#Part-2:-Preprocessing-and-Exploratory-Data-Anlysis)\n",
    "\n",
    "> #### [Part 3: Establishing a Hypothesis](#Part-3:-Establishing-a-Hypothesis)\n",
    "\n",
    "> #### [Part 4: Machine Learning!](#Part-4:-Machine-Learning!)\n",
    "\n",
    "### Additional Material\n",
    "\n",
    "> #### [Appendix I: Why are our models maxing out at around 80%?](#Appendix-I:-Why-are-our-models-maxing-out-at-around-80%?)\n",
    "\n",
    "> #### [Appendix II: Resources and references to material we won't cover in detail](#Appendix-II:-Resources-and-references-to-material-we-won't-cover-in-detail)\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>\n",
    "\n",
    "## Part 0: Install Additional Required Software\n",
    "\n",
    "__Note:__To follow along with notebook, you will need to install the __xgboost__ package in your pyhton enviroment:\n",
    "\n",
    "try (in terminal):\n",
    "```\n",
    "$ conda install py-xgboost\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "try (directly in notebook)\n",
    "```\n",
    "!conda install py-xgboost --y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "___\n",
    "## Part 1: Import Packages + Libraries + Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# 7 machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling and fancy distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling and display options\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "plt.rcParams[ 'figure.figsize' ] = 10 , 6\n",
    "\n",
    "pd.set_option('display.max_columns', 100) # Print 100 Pandas columns\n",
    "\n",
    "# Special distribution plot (will be used later)\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "combine = [train_df, test_df]\n",
    "# combine is used to ensure whatever preprocessing is done on training data\n",
    "# is also done on test data\n",
    "\n",
    "# NOTE! When we change train_df or test_df the objects in combine \n",
    "# will also change (combine is only a pointer to the objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "___\n",
    "## Part 2: Exploring and Preprocessing the Data\n",
    "**Data descriptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/Titanic_Variable.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features/Variable names\n",
    "\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data statistics\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame information (null, data type etc)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(figsize=(13,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Pclass', 'Survived']].groupby(['Pclass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set?\n",
    "\n",
    "target_count = train_df['Survived'].value_counts()\n",
    "target_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ If the goal is Prediction, unbalanced data introduce bias into model. \n",
    "\n",
    "Balanced data are good for classification, but you loose information such as appearance frequencies -- which may affect accuracy metrics themselves, as well as production performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is base line for prediction accuracy?\n",
    "\n",
    "target_count[0]/(sum(target_count)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "> #### __Brief Remarks Regarding the Data__\n",
    "\n",
    "> * `PassengerId` is a random number (incrementing index) and thus does not contain any valuable information. \n",
    "\n",
    "> * `Survived, Passenger Class, Age, Siblings Spouses, Parents Children` and `Fare` are numerical values (no need to transform them) -- but, we might want to group them (i.e. create categorical variables). \n",
    "\n",
    "> * `Sex, Embarked` are categorical features that we need to map to integer values. `Name, Ticket` and `Cabin` might also contain valuable information.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping Redundant Data\n",
    "__Note:__ It is important to remove variables that convey information already captured by some other variable. Doing so removes the correlation, while also diminishing potential overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions of the train and test datasets\n",
    "\n",
    "print(\"Shapes Before: (train) (test) = \", \\\n",
    "      train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Ticket', 'Cabin', need to do it for both test and training\n",
    "\n",
    "train_df = train_df.drop(['PassengerId','Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "print(\"Shapes After: (train) (test) =\", train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are null values in the datasets\n",
    "\n",
    "print(train_df.isnull().sum())\n",
    "print()\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data we have at hand to create ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "____\n",
    "## Part 3: Establishing Hypotheses\n",
    "\n",
    "### _The Title of the person is a feature that can predict survival_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List example titles in Name column\n",
    "\n",
    "train_df.Name[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the Name column we will extract title of each passenger\n",
    "# and save that in a column in the dataset called 'Title'\n",
    "# if you want to match Titles or Names with any other expression\n",
    "# refer to this tutorial on regex in python:\n",
    "# https://www.tutorialspoint.com/python/python_reg_expressions.htm\n",
    "\n",
    "# Create column called Title\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.',\\\n",
    "                                                expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that our titles makes sense (by comparing to sex)\n",
    "\n",
    "pd.crosstab(train_df['Title'], train_df['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">___\n",
    "> #### __Jonkheer?__ \n",
    "\n",
    "> Most popular during medieval times, the title of _Jonkheer_ was given to a young and unmarried child of a high-ranking knight or nobleman -- __Considered the lowest rank of nobility.__\n",
    "\n",
    ">___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but for test set\n",
    "pd.crosstab(test_df['Title'], test_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see common titles like Miss, Mrs, Mr, Master are dominant, we will\n",
    "# correct some Titles to standard forms and replace the rarest titles \n",
    "# with single name 'Rare'\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].\\\n",
    "                  replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr',\\\n",
    "                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') #Mademoiselle\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') #Madame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have more logical (contemporary) titles, and fewer groups\n",
    "\n",
    "train_df[['Title', 'Survived']].groupby(['Title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the survival chance for each title\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Title\", data=train_df, order=[1,0])\n",
    "plt.xticks(range(2),['Made it','Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title dummy mapping: Map titles to binary dummy columns\n",
    "\n",
    "for dataset in combine:\n",
    "    binary_encoded = pd.get_dummies(dataset.Title)\n",
    "    newcols = binary_encoded.columns\n",
    "    dataset[newcols] = binary_encoded\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique variables for analysis (Title is generally bound to Name, so it's also dropped)\n",
    "\n",
    "train_df = train_df.drop(['Name', 'Title'], axis=1)\n",
    "test_df = test_df.drop(['Name', 'Title'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Sex column to binary (male = 0, female = 1) categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variable to numeric\n",
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex']. \\\n",
    "        map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values for the age column\n",
    "train_df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values for age\n",
    "We will now guess values of age based on sex (male / female) \n",
    "and socioeconomic class (1st, 2nd, 3rd) of the passenger.\n",
    "\n",
    "The row indicates the sex, male = 0, female = 1\n",
    "\n",
    "> __IDEA:__ Wealth (indicated by class accomodation), as well as Gender, historically are indicative of age. \n",
    "\n",
    "> This approach gives us a more refined estimate than only taking the median / mean, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty array  for later use\n",
    "\n",
    "guess_ages = np.zeros((2,3),dtype=int) \n",
    "guess_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA's for the Age columns\n",
    "# with \"qualified guesses\"\n",
    "\n",
    "for idx,dataset in enumerate(combine):  \n",
    "    # method adds a counter to an iterable and returns it in a form of enumerate object.     \n",
    "    if idx==0:\n",
    "        print('Working on Training Data set\\n')\n",
    "    else:\n",
    "        print('-'*35)\n",
    "        print('Working on Test Data set\\n')\n",
    "    \n",
    "    print('Guess values of age based on sex and pclass of the passenger...')\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0,3):\n",
    "            guess_df = dataset[(dataset['Sex'] == i) \\\n",
    "                        & (dataset['Pclass'] == j+1)]['Age'].dropna()\n",
    "\n",
    "            # Extract the median age for this group\n",
    "            # (less sensitive) to outliers\n",
    "            age_guess = guess_df.median()\n",
    "          \n",
    "            # Convert random age float to int\n",
    "            guess_ages[i,j] = int(age_guess)\n",
    "    \n",
    "            \n",
    "    print('Guess_Age table:\\n',guess_ages)\n",
    "    print ('\\nAssigning age values to NAN age values in the dataset...')\n",
    "    \n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) \\\n",
    "                    & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n",
    "                    \n",
    "\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    print()\n",
    "print('Done! \\n\\n\\n')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split age into bands / categorical ranges and look at survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bands\n",
    "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n",
    "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False)\\\n",
    "                    .mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of suvival relative to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived \n",
    "# or did not survive\n",
    "\n",
    "plot_distribution( train_df , var = 'Age' , target = 'Survived' ,\\\n",
    "                  row = 'Sex' )\n",
    "\n",
    "# Recall: {'male': 0, 'female': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Age column to\n",
    "# map Age ranges (AgeBands) to integer values of categorical type \n",
    "\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 3\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 4\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']=5\n",
    "train_df.head()\n",
    "\n",
    "# Note we could just run \n",
    "# dataset['Age'] = pd.cut(dataset['Age'], 5,labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove AgeBand column\n",
    "\n",
    "train_df = train_df.drop(['AgeBand'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Party Size\n",
    "\n",
    "How did the number of people the person traveled with impact the chance of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp = Number of Sibling / Spouses\n",
    "# Parch = Parents / Children\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    \n",
    "# Survival chance against FamilySize\n",
    "\n",
    "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True) \\\n",
    "                                .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"FamilySize\", data=train_df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable if the person was alone or not\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the binary IsAlone feature for further analysis\n",
    "\n",
    "for df in combine:\n",
    "    df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new features based on intuitive combinations\n",
    "# Here is an example when we say that the age times socioclass is a determinant factor\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n",
    "\n",
    "train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Age*Class', 'Survived']].groupby(['Age*Class'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Port the person embarked from\n",
    "Let's see how that influences chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"data/images/titanic_voyage_map.png\">\n",
    ">___\n",
    "\n",
    "> #### __Interesting Fact:__ \n",
    "\n",
    "> Third Class passengers were the first to board, with First and Second Class passengers following up to an hour before departure. \n",
    "\n",
    "> Third Class passengers were inspected for ailments and physical impairments that might lead to their being refused entry to the United States, while First Class passengers were personally greeted by Captain Smith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace Nan value in 'Embarked', we will use the mode\n",
    "# in 'Embaraked'. This will give us the most frequent port \n",
    "# the passengers embarked from\n",
    "\n",
    "freq_port = train_df['Embarked'].dropna().mode()[0]\n",
    "print('Most frequent port of Embarkation:',freq_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN 'Embarked' Values in the datasets\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival relative to port of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True) \\\n",
    "                    .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of relationship between survival and origin \n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Embarked\", data=train_df, order=[1,0])\n",
    "plt.xticks(range(2),['Made it!', 'Deceased']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical dummy variables for Embarked values\n",
    "\n",
    "for dataset in combine:\n",
    "    binary_encoded = pd.get_dummies(dataset.Embarked)\n",
    "    newcols = binary_encoded.columns\n",
    "    dataset[newcols] = binary_encoded\n",
    "\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Embarked\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.drop('Embarked', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare Amount\n",
    "What is the relationship between _Fare_ and survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA values in the Fares column with the median\n",
    "\n",
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Fare'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q cut will find ranges equal to the quartile of the data\n",
    "\n",
    "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n",
    "train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical values for the models to generalize\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Fare']=pd.qcut(train_df['Fare'],4,labels=np.arange(4))\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "train_df[['Fare','FareBand']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop FareBand\n",
    "\n",
    "train_df = train_df.drop(['FareBand'], axis=1) \n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished -- Preprocessing Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are approximately on the same scale\n",
    "# no need for feature engineering / normalization\n",
    "\n",
    "train_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: View the correlation between features in our processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrelated features are generally more powerful predictors\n",
    "\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train_df.corr().round(2)\\\n",
    "            ,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \\\n",
    "            linecolor='white', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "___\n",
    "## Part 4: Machine Learning!\n",
    "Now we will Model, Predict, and Choose from algorithms for classification. \n",
    "We will try using different classifiers to model and predict. \n",
    "\n",
    "We ultimately will choose the best model from:\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN) \n",
    "3. Support Vector Machines (SVM)\n",
    "4. Perceptron\n",
    "5. XGBoost\n",
    "6. Random Forest\n",
    "7. Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"Survived\", axis=1) # Training & Validation data\n",
    "Y = train_df[\"Survived\"]              # Response / Target Variable\n",
    "\n",
    "X_submission  = test_df.drop(\"PassengerId\", axis=1).copy()\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set so that we validate on 20% of the data\n",
    "# Note that our algorithms will never have seen the validation \n",
    "# data during training. This is to evaluate how good our estimators are.\n",
    "\n",
    "np.random.seed(42) # set random seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> ### Scikit-Learn general ML workflow\n",
    "> 1. __Instantiate__ model object\n",
    "> 2. __Fit__ model to training data\n",
    "> 3. __Predict & Evaluate__ predict output for data not used during training and compare predicitons against true output values to form an accuracy measure.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Modeling Approaches (Algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()                                # instantiate\n",
    "logreg.fit(X_train, Y_train)                                 # fit\n",
    "Y_pred = logreg.predict(X_val)                               # predict\n",
    "acc_logreg = sum(Y_pred == Y_val)/len(Y_val)*100                # evaluate\n",
    "\n",
    "print('Logistic Regression labeling accuracy:', str(round(acc_logreg,2)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also use scikit learn's method score\n",
    "# that predicts and then compares to validation set labels\n",
    "acc_log_2 = logreg.score(X_val, Y_val)                       # evaluate\n",
    "\n",
    "print('Logistic Regression using built-in method:', str(round(acc_log_2*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)                  # instantiate\n",
    "knn.fit(X_train, Y_train)                                    # fit\n",
    "acc_knn = knn.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('K-Nearest Neighbors labeling accuracy:', str(round(acc_knn*100,2)),'%')                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines Classifier (non-linear kernel)\n",
    "svc = SVC()                                                  # instantiate\n",
    "svc.fit(X_train, Y_train)                                    # fit\n",
    "acc_svc = svc.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('Support Vector Machines labeling accuracy:', str(round(acc_svc*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()                                    # instantiate \n",
    "perceptron.fit(X_train, Y_train)                             # fit\n",
    "acc_perceptron = perceptron.score(X_val, Y_val)              # predict + evalaute\n",
    "\n",
    "print('Perceptron labeling accuracy:', str(round(acc_perceptron*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(n_estimators=1000, eval_metric='mlogloss')             # instantiate\n",
    "gradboost.fit(X_train, Y_train)                              # fit\n",
    "acc_xgboost = gradboost.score(X_val, Y_val)                  # predict + evalute\n",
    "\n",
    "print('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=500)   # instantiate\n",
    "random_forest.fit(X_train, Y_train)                         # fit\n",
    "acc_rf = random_forest.score(X_val, Y_val)                  # predict + evaluate\n",
    "\n",
    "print('K-Nearest Neighbors labeling accuracy:', str(round(acc_rf*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "\n",
    "model = Sequential()\n",
    "model.add( Dense(units=300, activation='relu', input_shape=(14,) ))\n",
    "model.add( Dense(units=100, activation='relu'))\n",
    "model.add( Dense(units=50, activation='relu'))\n",
    "model.add( Dense(units=1, activation='sigmoid') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam',\\\n",
    "              metrics = ['accuracy'])\n",
    "model.fit(X_train.values, Y_train.values, epochs = 50, batch_size= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model Accuracy on test set\n",
    "print('Neural Network accuracy:',str(round(model.evaluate(X_val.values, \\\n",
    "                Y_val.values, batch_size=50,verbose=False)[1]*100,2)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance scores in the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at importnace of features for random forest\n",
    "\n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print ('Training accuracy Random Forest:',model.score( X , y ))\n",
    "\n",
    "plot_model_var_imp(random_forest, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "# Define the ensemble model\n",
    "ensemble = VotingClassifier(estimators=[('rf', rf), ('knn', knn), ('svm', svm)], voting='soft')\n",
    "\n",
    "# Fit the ensemble model to the data\n",
    "ensemble.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = ensemble.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(Y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing & evaluating a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_estimators': [250, 500, 750],\n",
    "              'max_depth': [10, 20],\n",
    "              'min_samples_split': [2, 10],\n",
    "              'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels of the val set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(Y_val, y_pred)\n",
    "prec = precision_score(Y_val, y_pred)\n",
    "recall = recall_score(Y_val, y_pred)\n",
    "f1 = f1_score(Y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(Y_val, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compete on Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to create a Kaggle submission with a Random Forest Classifier\n",
    "Y_submission = best_model.predict(X_submission)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_submission\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "___\n",
    "___\n",
    "\n",
    "## Appendix I: Why are our models maxing out at around 82%?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __John Jacob Astor__\n",
    "\n",
    "<img src= \"data/images/john-jacob-astor.jpg\"> \n",
    "\n",
    "John Jacob Astor perished in the disaster even though our model predicted he would survive. Astor was the wealthiest person on the Titanic -- his ticket fare was valued at over 35,000 USD in 2016 -- it seems likely that he would have been among of the approximatelly 35 percent of men in first class to survive. However, this was not the case: although his pregnant wife survived, John Jacob Astor’s body was recovered a week later, along with a gold watch, a diamond ring with three stones, and no less than 92,481 USD (2016 value) in cash.\n",
    "\n",
    "<br >\n",
    "\n",
    "\n",
    "#### __Olaus Jorgensen Abelseth__\n",
    "\n",
    "<img src= \"data/images/olaus-jorgensen-abelseth.jpg\">\n",
    "\n",
    "Abelseth was a 25-year-old Norwegian sailor, a man in 3rd class, and not expected to survive by classifier. However, once the ship sank, he survived by swimming for 20 minutes in the frigid North Atlantic water before joining other survivors on a waterlogged collapsible boat.\n",
    "\n",
    "Abelseth got married three years later, settled down as a farmer in North Dakota, had 4 kids, and died in 1980 at the age of 94.\n",
    "\n",
    "<br >\n",
    "\n",
    "### __Key Takeaway__ \n",
    "\n",
    "As engineers and busines professionals, we are trained to as ourselves, what could we do to improve on an 80 percent average. As it is often the case, it’s easy to forget that these data points represent real people. Each time our model was wrong we should be glad -- in such misclasifications we will likely find incredible stories of human nature and courage triumphing over extremely difficult odds. \n",
    "\n",
    "__It is important to never lose sight of the human element when analyzing data that deals with people.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "___\n",
    "## Appendix II: Resources and references to material we won't cover in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Gradient Boosting:** http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "> * **Jupyter Notebook (tutorial):** https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "\n",
    "> * **K-Nearest Neighbors (KNN):** https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26\n",
    "\n",
    "> * **Logistic Regression:** https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\n",
    "\n",
    "> * **Naive Bayes:** http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "> * **Perceptron:** http://aass.oru.se/~lilien/ml/seminars/2007_02_01b-Janecek-Perceptron.pdf\n",
    "\n",
    "> * **Random Forest:** https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "\n",
    "> * **Support Vector Machines (SVM):** https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "\n",
    "\n",
    "<br>\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
