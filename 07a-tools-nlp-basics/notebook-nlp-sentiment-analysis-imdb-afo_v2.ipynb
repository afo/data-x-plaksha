{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "\n",
    "# Data-X Notebook: NLP & NLTK\n",
    "#### Using Natural Language Processing in Python to do sentiment analysis on IMDB movie reviews\n",
    "\n",
    "Author: Alexander Fred Ojala\n",
    "\n",
    "Source: \n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "- https://github.com/rasbt/machine-learning-book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "### - [Part 0: Pre-Setup & Data description](#sec0)\n",
    "### - [Part 1: Explore the Data](#sec1)\n",
    "### - [Part 2: NLTK intro (1 review) cleaning, tokenizing, stemming, lemmatization, stopwords](#sec2)\n",
    "### - [Part 3: Preparing the data set for classification](#sec3)\n",
    "### - [Part 4: Sentiment Classification w scikit-learn, Feature vectors & Bag of Words model](#sec4)\n",
    "#### - [Appendix: Submitting to Kaggle & possible extensions](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec0'></div>\n",
    "\n",
    "## Part 0: Pre-Setup & Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "\n",
    "You can download the data (labeledTrainData.tsv.zip) here: https://www.kaggle.com/c/word2vec-nlp-tutorial/data, place it in your working directory & unzip the file. (It is placed in the data folder if you cloned the Github repo)\n",
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledTrainData.tsv  testData.tsv\r\n"
     ]
    }
   ],
   "source": [
    "%ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tsentiment\treview\r\n",
      "\"5814_8\"\t1\t\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\r\n",
      "\"2381_9\"\t1\t\"\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\r\n",
      "\"7759_3\"\t0\t\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n4 data/labeledTrainData.tsv \n",
    "# use bash command to see data set structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data to a Pandas data frame\n",
    "# Use header = 0 (first line contains col names)\n",
    "# use delimiter=\\t (columns are separated by tabs),\n",
    "# use quoting=3 (Python will ignore doubled quotes)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         25000 non-null  object\n",
      " 1   sentiment  25000 non-null  int64 \n",
      " 2   review     25000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() # no NaN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: How many movie reviews are positive and how many are negative in labeledTrainData.tsv? Do we have balance between the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "\n",
      "Number of Data Samples for every label output. 1=postive, 2=negative:\n",
      "1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. \n",
    "print(train.shape)\n",
    "print()\n",
    "print('Number of Data Samples for every label output. 1=postive, 2=negative:')\n",
    "print(train.sentiment.value_counts())\n",
    "\n",
    "train.sentiment.hist(); #class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: What is the average length of all the reviews (string length)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of the reviews are:\n",
      "1329.71056\n"
     ]
    }
   ],
   "source": [
    "# 2. Apply length function to the review column\n",
    "lengths = train['review'].apply(len)\n",
    "\n",
    "print('Average character length of the reviews are:')\n",
    "print (np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec2'></div>\n",
    "\n",
    "# Part 2: \n",
    "## NLTK intro (1 review) cleaning, tokenizing, stemming, lemmatization, stopwords\n",
    "\n",
    "Let's explore NLP by looking at the third review in the training data set, i.e. `train['review'][2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DOWNLOAD NLTK CORPORA\n",
    "Run the below command in an empty cell\n",
    "```python\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "Then, write \n",
    "\n",
    "    stopwords punkt wordnet averaged_perceptron_tagger\n",
    "\n",
    "to download the packages needed ie: `Corpora / stopwords` and `Models / punkt` and `Models / Averaged Perceptron` and `Corpora / wordnet`.\n",
    "\n",
    "Don't forget to interrupt the kernel when you're done downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/afo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/afo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/afo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/afo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'punkt', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"\n"
     ]
    }
   ],
   "source": [
    "review3 = train['review'][2] # the review used for initial analysis\n",
    "print(review3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the review\n",
    "\n",
    "First we would like to clean up the reviews. As you can see many interviews contain \\ characters in front of quotation symobols, \"`<br/>` tags, numbers, abbrevations etc.\n",
    "\n",
    "### 1: Remove all the HTML tags in the third review, by creating a beatifulsoup object and then using the `.text` method. Save results in variable `review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "review3 = bs.BeautifulSoup(review3,features='lxml').text # removes HTML tags\n",
    "print(review3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Import NLTK's sent_tokenizer and count the number of sentences in review 3 \n",
    "The review should be cleaned from HTML tags. To import sent_tokenizer use: `from nltk.tokenize import sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park .',\n",
       " \"A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon .\",\n",
       " 'Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger .',\n",
       " 'In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons.',\n",
       " 'The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing.',\n",
       " \"The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators.\",\n",
       " 'Furthermore a third Sabretooth more dangerous and slow stalks its victims.The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls .',\n",
       " 'And it packs a ridiculous final deadly scene.',\n",
       " 'No for small kids by realistic,gory and violent attack scenes .',\n",
       " 'Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle.',\n",
       " 'This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films.',\n",
       " 'Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ).',\n",
       " 'Rating : Below average, bottom of barrel.\"']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sent_tokenize(review3)))\n",
    "sent_tokenize(review3) # doesn't really split all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if it does a better job if we add space after every period\n",
    "review3 = review3.replace('.','. ')\n",
    "\n",
    "print(len(sent_tokenize(review3)), end='\\n\\n') # number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park .\n",
      "\n",
      "A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon .\n",
      "\n",
      "Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.\n",
      "\n",
      "Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger .\n",
      "\n",
      "In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons.\n",
      "\n",
      "The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing.\n",
      "\n",
      "The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators.\n",
      "\n",
      "Furthermore a third Sabretooth more dangerous and slow stalks its victims.\n",
      "\n",
      "The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.\n",
      "\n",
      "The story provides exciting and stirring entertainment but it results to be quite boring .\n",
      "\n",
      "The giant animals are majority made by computer generator and seem totally lousy .\n",
      "\n",
      "Middling performances though the players reacting appropriately to becoming food.\n",
      "\n",
      "Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls .\n",
      "\n",
      "And it packs a ridiculous final deadly scene.\n",
      "\n",
      "No for small kids by realistic,gory and violent attack scenes .\n",
      "\n",
      "Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.\n",
      "\n",
      "000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle.\n",
      "\n",
      "This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films.\n",
      "\n",
      "Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ).\n",
      "\n",
      "Rating : Below average, bottom of barrel. \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print all sentences on a new line\n",
    "for sent in sent_tokenize(review3):\n",
    "    print(sent, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Remove all punctuation and special characters from the third review.\n",
    "We can do this using Regular Expression - package `re`. Regular expression define search patterns for text that can be used to search, replace, substitute etc. certain patterns in text data.\n",
    "\n",
    "We'll use regular expression to only look for alphabetical characters, `[^a-zA-Z]`. Save results in variable `review3`\n",
    "\n",
    "Intro to Regex: http://www.aivosto.com/vbtips/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film starts with a manager Nicholas Bell giving welcome investors Robert Carradine to Primal Park   A secret project mutating a primal animal using fossilized DNA like Jurassik Park and some scientists resurrect one of natures most fearsome predators the Sabretooth tiger or Smilodon   Scientific ambition turns deadly however and when the high voltage fence is opened the creature escape and begins savagely stalking its prey  the human visitors  tourists and scientific Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large prehistorical animals which are deadlier and bigger   In addition  a security agent Stacy Haiduk and her mate Brian Wimmer fight hardly against the carnivorous Smilodons  The Sabretooths themselves  of course are the real star stars and they are astounding terrifyingly though not convincing  The giant animals savagely are stalking its prey and the group run afoul and fight against one natures most fearsome predators  Furthermore a third Sabretooth more dangerous and slow stalks its victims The movie delivers the goods with lots of blood and gore as beheading hairraising chillsfull of scares when the Sabretooths appear with mediocre special effects The story provides exciting and stirring entertainment but it results to be quite boring  The giant animals are majority made by computer generator and seem totally lousy  Middling performances though the players reacting appropriately to becoming food Actors give vigorously physical performances dodging the beasts runningbound and leaps or dangling over walls   And it packs a ridiculous final deadly scene  No for small kids by realisticgory and violent attack scenes   Other films about Sabretooths or Smilodon are the following  Sabretoothby James R Hickox with Vanessa Angel David Keith and John Rhys Davies and the much better   BC by Roland Emmerich with with Steven Strait Cliff Curtis and Camilla Belle  This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films  Miller is an Australian director usually working for television Tidal wave Journey to the center of the earth and many others and occasionally for cinema  The man from Snowy river Zeus and RoxanneRobinson Crusoe   Rating  Below average bottom of barrel \n"
     ]
    }
   ],
   "source": [
    "review3 = re.sub('[^a-zA-Z ]' ,'',review3)\n",
    "print(review3) # remove special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Convert all the letters to lower case, split the string so that every word is one element in a list\n",
    "Note: When we split the strings into words that process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the film starts with a manager nicholas bell giving welcome investors robert carradine to primal park   a secret project mutating a primal animal using fossilized dna like jurassik park and some scientists resurrect one of natures most fearsome predators the sabretooth tiger or smilodon   scientific ambition turns deadly however and when the high voltage fence is opened the creature escape and begins savagely stalking its prey  the human visitors  tourists and scientific meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large prehistorical animals which are deadlier and bigger   in addition  a security agent stacy haiduk and her mate brian wimmer fight hardly against the carnivorous smilodons  the sabretooths themselves  of course are the real star stars and they are astounding terrifyingly though not convincing  the giant animals savagely are stalking its prey and the group run afoul and fight against one natures most fearsome predators  furthermore a third sabretooth more dangerous and slow stalks its victims the movie delivers the goods with lots of blood and gore as beheading hairraising chillsfull of scares when the sabretooths appear with mediocre special effects the story provides exciting and stirring entertainment but it results to be quite boring  the giant animals are majority made by computer generator and seem totally lousy  middling performances though the players reacting appropriately to becoming food actors give vigorously physical performances dodging the beasts runningbound and leaps or dangling over walls   and it packs a ridiculous final deadly scene  no for small kids by realisticgory and violent attack scenes   other films about sabretooths or smilodon are the following  sabretoothby james r hickox with vanessa angel david keith and john rhys davies and the much better   bc by roland emmerich with with steven strait cliff curtis and camilla belle  this motion picture filled with bloody moments is badly directed by george miller and with no originality because takes too many elements from previous films  miller is an australian director usually working for television tidal wave journey to the center of the earth and many others and occasionally for cinema  the man from snowy river zeus and roxannerobinson crusoe   rating  below average bottom of barrel '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review3 = review3.lower()\n",
    "review3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'film', 'starts', 'with', 'a', 'manager', 'nicholas', 'bell', 'giving', 'welcome']\n"
     ]
    }
   ],
   "source": [
    "review3_words = review3.split()\n",
    "print(review3_words[:10]) # tokenize and lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Stemming\n",
    "Use NLTK's PorterStemmer (`from nltk.stem import PorterStemmer`). Create a new Porter stemmer (`stemmer = PorterStemmer()`) and run it on every word in `review3_words`, print the results as one string (don't overwrite the `review3_words` variable from 4). \n",
    "\n",
    "**What does the PorterStemmer do?**\n",
    "PorterStemmer tries treating similar words as the same (e.g. give, gives, given = give).\n",
    "\n",
    "**Stemming (paraphrased from Wiki):** Stemming reduces words to their word stem, base or root form (i.e. similar words, with the same stem, will be interpreted as the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manag'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer() #initialize Porter Stemmer object\n",
    "\n",
    "ps.stem('managers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film start with a manag nichola bell give welcom investor robert carradin to primal park a secret project mutat a primal anim use fossil dna like jurassik park and some scientist resurrect one of natur most fearsom predat the sabretooth tiger or smilodon scientif ambit turn deadli howev and when the high voltag fenc is open the creatur escap and begin savag stalk it prey the human visitor tourist and scientif meanwhil some youngster enter in the restrict area of the secur center and are attack by a pack of larg prehistor anim which are deadlier and bigger in addit a secur agent staci haiduk and her mate brian wimmer fight hardli against the carnivor smilodon the sabretooth themselv of cours are the real star star and they are astound terrifyingli though not convinc the giant anim savag are stalk it prey and the group run afoul and fight against one natur most fearsom predat furthermor a third sabretooth more danger and slow stalk it victim the movi deliv the good with lot of blood and gore as behead hairrais chillsful of scare when the sabretooth appear with mediocr special effect the stori provid excit and stir entertain but it result to be quit bore the giant anim are major made by comput gener and seem total lousi middl perform though the player react appropri to becom food actor give vigor physic perform dodg the beast runningbound and leap or dangl over wall and it pack a ridicul final deadli scene no for small kid by realisticgori and violent attack scene other film about sabretooth or smilodon are the follow sabretoothbi jame r hickox with vanessa angel david keith and john rhi davi and the much better bc by roland emmerich with with steven strait cliff curti and camilla bell thi motion pictur fill with bloodi moment is badli direct by georg miller and with no origin becaus take too mani element from previou film miller is an australian director usual work for televis tidal wave journey to the center of the earth and mani other and occasion for cinema the man from snowi river zeu and roxannerobinson cruso rate below averag bottom of barrel\n"
     ]
    }
   ],
   "source": [
    "# stem review three\n",
    "ps_stems = []\n",
    "for w in review3_words:\n",
    "    ps_stems.append(ps.stem(w))\n",
    "\n",
    "print(' '.join(ps_stems)) # add all the stemmed words to one string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Part of Speech tagging\n",
    "Now we want to Part Of Speech (POS)) tag the third movie review. We will use POS labeling, also called grammatical tagging. To do this import `from nltk.tag import pos_tag`. \n",
    "\n",
    "When you use `pos_tag` on a word it returns a token-tag pair in the form of a tuple. In NLTK's Penn Treebank POS, the abbreviation (tag) for an adjective is JJ and NN for singular nouns. \n",
    "\n",
    "Let's count the number of singular nouns (NN) and adjectives (JJ) in `review3_words` using NLTK's pos_tag. A list of the Penn Treebank pos_tag's can be found here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('run', 'VB'), ('green', 'JJ'), ('ball', 'NN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['run','green','ball'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('film', 'NN'),\n",
       " ('starts', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('manager', 'NN'),\n",
       " ('nicholas', 'NN'),\n",
       " ('bell', 'NN'),\n",
       " ('giving', 'VBG'),\n",
       " ('welcome', 'JJ')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.\n",
    "token_tag = pos_tag(review3_words)\n",
    "token_tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nouns: 83\n",
      "Number of adjectives: 44\n"
     ]
    }
   ],
   "source": [
    "# Cell that count nouns (NN) and adjectives (JJ) in review3\n",
    "NN_count = 0\n",
    "JJ_count = 0\n",
    "\n",
    "for pair in token_tag:\n",
    "    tag = pair[1]\n",
    "    if tag == 'JJ':\n",
    "        JJ_count+=1\n",
    "    elif tag == 'NN':\n",
    "        NN_count+=1\n",
    "print('Number of nouns:', NN_count)\n",
    "print('Number of adjectives:', JJ_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lemmatizing\n",
    "An even more sophisticated operation than stemming using the PorterStemmer is called lemmatizing. \n",
    "\n",
    "Lemmatizing, in contrast to stemming, does not create non-existent words and converts words to their synonyms. In order to use lemmatizing we need to define the wordnet POS tag. A function that takes in a POS Penn Treebank tag and converts it to a wordnet tag and then lemmatizes words in a string has been given written below. \n",
    "\n",
    "**Lemmatization (paraphrased from Wiki):** Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "We can use this to print the lemmatized third movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/afo/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize('ran','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film start with a manager nicholas bell give welcome investor robert carradine to primal park a secret project mutate a primal animal use fossilized dna like jurassik park and some scientist resurrect one of nature most fearsome predator the sabretooth tiger or smilodon scientific ambition turn deadly however and when the high voltage fence be open the creature escape and begin savagely stalk it prey the human visitor tourist and scientific meanwhile some youngster enter in the restricted area of the security center and be attack by a pack of large prehistorical animal which be deadlier and big in addition a security agent stacy haiduk and her mate brian wimmer fight hardly against the carnivorous smilodons the sabretooths themselves of course be the real star star and they be astound terrifyingly though not convince the giant animal savagely be stalk it prey and the group run afoul and fight against one natures most fearsome predator furthermore a third sabretooth more dangerous and slow stalk it victim the movie deliver the good with lot of blood and gore a behead hairraising chillsfull of scare when the sabretooths appear with mediocre special effect the story provide exciting and stirring entertainment but it result to be quite bore the giant animal be majority make by computer generator and seem totally lousy middle performance though the player react appropriately to become food actor give vigorously physical performance dodge the beast runningbound and leap or dangle over wall and it pack a ridiculous final deadly scene no for small kid by realisticgory and violent attack scenes other film about sabretooths or smilodon be the following sabretoothby james r hickox with vanessa angel david keith and john rhys davy and the much good bc by roland emmerich with with steven strait cliff curtis and camilla belle this motion picture fill with bloody moment be badly direct by george miller and with no originality because take too many element from previous film miller be an australian director usually work for television tidal wave journey to the center of the earth and many others and occasionally for cinema the man from snowy river zeus and roxannerobinson crusoe rating below average bottom of barrel\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. StopWords\n",
    "\n",
    "Lastly we will remove common words that don't carry much information. These are called stopwords. \n",
    "\n",
    "In English they could for example be 'am', 'are', 'and' etc. \n",
    "\n",
    "To import NLTK's list of stopwords you need to download the stopword corpora (`import nltk` and then `nltk.download()` if you don't have it). \n",
    "\n",
    "When that is done run `from nltk.corpus import stopwords` and create a variable for English stopwords with `eng_stopwords = stopwords.words('english')`. Use the list of English stopwords to remove all the stopwords from your list of words in the third movie review, i.e. `review3_words`. Print `review3_words` without stopwords, count the number of stopwords removed and print them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW WITHOUT STOPWORDS:\n",
      "film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one natures fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large prehistorical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one natures fearsome predators furthermore third sabretooth dangerous slow stalks victims movie delivers goods lots blood gore beheading hairraising chillsfull scares sabretooths appear mediocre special effects story provides exciting stirring entertainment results quite boring giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts runningbound leaps dangling walls packs ridiculous final deadly scene small kids realisticgory violent attack scenes films sabretooths smilodon following sabretoothby james r hickox vanessa angel david keith john rhys davies much better bc roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxannerobinson crusoe rating average bottom barrel\n",
      "\n",
      "Stop words removed ['the', 'with', 'a', 'to', 'a', 'a', 'and', 'some', 'of', 'most', 'the', 'or', 'and', 'when', 'the', 'is', 'the', 'and', 'its', 'the', 'and', 'some', 'in', 'the', 'of', 'the', 'and', 'are', 'by', 'a', 'of', 'which', 'are', 'and', 'in', 'a', 'and', 'her', 'against', 'the', 'the', 'themselves', 'of', 'are', 'the', 'and', 'they', 'are', 'not', 'the', 'are', 'its', 'and', 'the', 'and', 'against', 'most', 'a', 'more', 'and', 'its', 'the', 'the', 'with', 'of', 'and', 'as', 'of', 'when', 'the', 'with', 'the', 'and', 'but', 'it', 'to', 'be', 'the', 'are', 'by', 'and', 'the', 'to', 'the', 'and', 'or', 'over', 'and', 'it', 'a', 'no', 'for', 'by', 'and', 'other', 'about', 'or', 'are', 'the', 'with', 'and', 'and', 'the', 'by', 'with', 'with', 'and', 'this', 'with', 'is', 'by', 'and', 'with', 'no', 'because', 'too', 'from', 'is', 'an', 'for', 'to', 'the', 'of', 'the', 'and', 'and', 'for', 'the', 'from', 'and', 'below', 'of']\n",
      "\n",
      "NUMBER OF STOPWORDS REMOVED: 132\n"
     ]
    }
   ],
   "source": [
    "review3_wo_stopwords = [w for w in review3_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in review3_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print('REVIEW WITHOUT STOPWORDS:')\n",
    "print(' '.join(review3_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec3'></div>\n",
    "\n",
    "# Part 3: Preparing the data set for classification\n",
    "\n",
    "In this part we'll put everything that we've learned so far together. This so that we can use our reviews to make accurate predicitons on their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def review_cleaner(review):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "    \n",
    "    #3. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #5. Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review = ' '.join(review+emoticons)\n",
    "    # add emoticons to the end\n",
    "\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create original, Porter Stemmed, and Lemmatized data sets\n",
    "    \n",
    "**Create three lists `review_clean_original`, `review_clean_ps` and `review_clean_wnl`.**\n",
    "\n",
    "- `review_clean_original` contains all the reviews from the train DataFrame, that have been cleaned by the function `review_cleaner` defined above.\n",
    "\n",
    "- `review_clean_ps` applies the PorterStemmer to the reviews in `review_clean_original`. **Note:** NLTK version 3.2.2 crashes when trying to use the PorterStemming on the string 'oed' (known bug). Therefore, use an if statement to skip just that specific string/word.\n",
    "\n",
    "- `review_clean_wnl` contains words that have been lemmatized using NLTK's WordNetLemmatizer on the words in the list `review_clean_original`.\n",
    "    \n",
    "**Note: This can take more than 10minutes to run on a laptop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 5.56 s, sys: 320 ms, total: 5.88 s\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 30.3 s, sys: 193 ms, total: 30.5 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Porter stemming on the results in review_clean_original\n",
    "\n",
    "review_clean_ps = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    ps_stems = []\n",
    "    for w in review_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        ps_stems.append(ps.stem(w))\n",
    "    \n",
    "    review_clean_ps.append(' '.join(ps_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 1min 22s, sys: 582 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lemmatizer\n",
    "\n",
    "review_clean_wnl = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    \n",
    "    wnl_stems = []\n",
    "    token_tag = pos_tag(review_clean_original[i].split())\n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "\n",
    "    review_clean_wnl.append(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec4'></div>\n",
    "\n",
    "# Part 4: Sentiment Classification w scikit-learn, Feature vectors & Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: Explanation\n",
    "\n",
    "Derived from source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "We will now use scikit-learn to create numeric representations of the words in the reviews, using a method called Bag of Words. You can see this as learning a vocabulary from all the reviews and counting how many times a word appears in the reviews. For example, if we have two sentences:\n",
    "\n",
    "**Sentence 1:** \"cool students study cool data science\"\n",
    "\n",
    "**Sentence 2:** \"to know data science study data science\"\n",
    "\n",
    "The vocabulary of these two sentences can be summarized in a dictionary:\n",
    "\n",
    "    { cool, students, study, data, science, to, know }\n",
    "\n",
    "The bags of words count the number of times each word occur in a sentence. In Sentence 1, \"cool\" appears twice, and \"students\", \"study\", \"data\", and \"science\" appear once. The feature vector for Sentence 1 is:\n",
    "\n",
    "    Sentence 1: { 2, 1, 1, 1, 1, 0, 0 }\n",
    "\n",
    "    Sentence 2: { 0, 0, 1, 2, 2, 1, 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `fit` method in scikit-learns `CountVectorizer` creates the vocabulary of the bag-of-words model and fits the function to the data set you pass in.\n",
    "\n",
    "The `transform` method in `CountVectorizer` transforms text input to a feature matrix, where the rows are the document inputs and the columns are the words / feature in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example code BoW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# or from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sent1 = \"cool students study cool data science\"\n",
    "sent2 = \"to know data science study data science\"\n",
    "\n",
    "vect = CountVectorizer() #instantiate\n",
    "vect2 = TfidfVectorizer()\n",
    "\n",
    "sents = np.array([sent1,sent2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cool students study cool data science',\n",
       "       'to know data science study data science'], dtype='<U39')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect2.fit(sents);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the vocabulary (and position in feature matrix):\n",
      "\n",
      "{'cool': 0, 'students': 4, 'study': 5, 'data': 1, 'science': 3, 'to': 6, 'know': 2}\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words in the vocabulary (and position in feature matrix):\\n')\n",
    "print(vect.vocabulary_)\n",
    "\n",
    "# vocabulary for the BoW model is stored in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78333663, 0.27867523, 0.        , 0.27867523, 0.39166832,\n",
       "        0.27867523, 0.        ],\n",
       "       [0.        , 0.55575576, 0.39054766, 0.55575576, 0.        ,\n",
       "        0.27787788, 0.39054766]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform to get feature vectors\n",
    "\n",
    "bag = vect2.transform(sents)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "# the rows corresponds to the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool', 'data', 'know', 'science', 'students', 'study', 'to']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names() # stored in the right places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>data</th>\n",
       "      <th>know</th>\n",
       "      <th>science</th>\n",
       "      <th>students</th>\n",
       "      <th>study</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cool students study cool data science</th>\n",
       "      <td>0.783337</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to know data science study data science</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555756</td>\n",
       "      <td>0.390548</td>\n",
       "      <td>0.555756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277878</td>\n",
       "      <td>0.390548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cool      data      know  \\\n",
       "cool students study cool data science    0.783337  0.278675  0.000000   \n",
       "to know data science study data science  0.000000  0.555756  0.390548   \n",
       "\n",
       "                                          science  students     study  \\\n",
       "cool students study cool data science    0.278675  0.391668  0.278675   \n",
       "to know data science study data science  0.555756  0.000000  0.277878   \n",
       "\n",
       "                                               to  \n",
       "cool students study cool data science    0.000000  \n",
       "to know data science study data science  0.390548  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put it in a DataFrame for interpretability\n",
    "\n",
    "pd.DataFrame(bag.toarray(), columns=vect.get_feature_names(), index=[sent1,sent2])\n",
    "\n",
    "# the number in the DataFrame is called Raw Term frequency raw term frequencies: \n",
    "# tf (t,d)—the number of times a term t occurs in a document d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying this strategy to the IMDB movie reviews\n",
    "\n",
    "The movie review data contains a lot of words. To limit the analysis we use the 5000 most frequent words from the cleaned reviews. To extract the bag of words features we will use scitkit-learn.\n",
    "\n",
    "The training data will be created by the `CountVectorizer` function from scikit-learn, and the training array will have 25000 rows (one for each review) and 5000 features (one for each vocabulary word).\n",
    "\n",
    "CountVectorizer can automatically handle text cleaning, but here we specify `None`, instead we did a step-by-step cleaning of the data in the earlier problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for review sentiment classification\n",
    "\n",
    "First split up the data set so that 80% are used as training samples (the first 20000 reviews and their sentiment) and 20% are used as validation samples (the last 5000 reviews and their sentiment). Use Random Forest to do numeric training on the features for the training samples from the Bag of Words and their respective sentiment labels for each review / feature vector. The number of trees is set to 50 as a default value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    review_clean_original, train['sentiment'], random_state=0, test_size=.2)\n",
    "\n",
    "\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 919 ms, sys: 21.3 ms, total: 940 ms\n",
      "Wall time: 944 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_features=5000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=5000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_features=5000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Transform the text data to feature\n",
    "# Only fit training data (to mimic real world)\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "# Check that it worked, \n",
    "# now we have fitted a model that can transform features\n",
    "# to sparse matrix representation\n",
    "\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000)\n",
      "(5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_bag.toarray().shape) # 20,000 reviews, 2,000 feartures. just as expected\n",
    "print(test_bag.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 37)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 126)\t1\n",
      "  (0, 142)\t2\n",
      "  (0, 145)\t1\n",
      "  (0, 147)\t1\n",
      "  (0, 162)\t1\n",
      "  (0, 194)\t2\n",
      "  (0, 205)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 315)\t1\n",
      "  (0, 327)\t2\n",
      "  (0, 335)\t1\n",
      "  (0, 368)\t1\n",
      "  (0, 395)\t1\n",
      "  (0, 411)\t1\n",
      "  (0, 436)\t1\n",
      "  (0, 475)\t1\n",
      "  (0, 480)\t1\n",
      "  (0, 485)\t1\n",
      "  :\t:\n",
      "  (19999, 3301)\t1\n",
      "  (19999, 3385)\t1\n",
      "  (19999, 3551)\t1\n",
      "  (19999, 3643)\t1\n",
      "  (19999, 3762)\t1\n",
      "  (19999, 3824)\t2\n",
      "  (19999, 3877)\t2\n",
      "  (19999, 3885)\t1\n",
      "  (19999, 3886)\t2\n",
      "  (19999, 3914)\t2\n",
      "  (19999, 3970)\t2\n",
      "  (19999, 4057)\t1\n",
      "  (19999, 4095)\t1\n",
      "  (19999, 4102)\t1\n",
      "  (19999, 4191)\t1\n",
      "  (19999, 4248)\t1\n",
      "  (19999, 4279)\t1\n",
      "  (19999, 4473)\t1\n",
      "  (19999, 4474)\t1\n",
      "  (19999, 4619)\t1\n",
      "  (19999, 4699)\t1\n",
      "  (19999, 4766)\t1\n",
      "  (19999, 4811)\t1\n",
      "  (19999, 4853)\t1\n",
      "  (19999, 4892)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(train_bag)) # sparse matrix representation\n",
    "\n",
    "print(train_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasify with Random Forest model\n",
    "\n",
    "* Fit a Random Forest model to our bagged data set in order to do the sentiment analysis on `review_clean_original` and print the **validation accuracy** by using `forest.predict(test_bag)` and then comparing the resulting sentiment predictions with the ones stored in `y_test`.\n",
    "\n",
    "*This can take 2-3 mins to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Initialize a Random Forest classifier with 50 trees\n",
    "# hyperparameter n_estimators always set in instantiation\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 s, sys: 29.8 ms, total: 3.94 s\n",
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the target variable\n",
    "\n",
    "forest = forest.fit(train_bag, y_train) # can take 20 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions) # 100% training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8394"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,valid_predictions) # 83% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2186,  362],\n",
       "       [ 441, 2011]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "# Is the number of False Positives and True negatives approx 50/50?\n",
    "metrics.confusion_matrix(y_test,valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 441)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test,valid_predictions).ravel()\n",
    "fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_predictions==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural born killers cinema cut r director cut nc unusual oliver stone picture read drugs filming needed explanation natural born killers risky mad film making get often strange psychotic artistic pictures natural born killers basically story two mass killers popularised glorified media great scene interviewer questions teenagers mickey mallory teenager says murder wrong mass murderer mickey mallory mickey describes situation frankenstein monster dr frankenstein dr frankenstein media turned monstrous killersmost oliver stone films examine flaws america country director loves admires guess natural born killers effect mass media technology obsessive nation americans world things mass killers bizarre situations killers played woody harrelson mickey juliette lewis mallory executed astonishingly two excellent actors step lives two interestingly brutal killers mickey mallory believe people worthy killing perhaps cruel theory social darwinism survival fittest mickey says interview prison species commit murder humans ravage species exploit environment script interesting questionable much film amounts sense making us think society human behaviour rather intensity hour bloodbath seen last hour film takes place maximum security prison see harsh realities prison life attitudes warden etc overfilling prisons maybe stone questioning future path society leading two interesting characters first reporter runs show america maniacs obsessed boosting ratings goes length capture story mickey mallory police officer scagnetti insane perhaps sadistic officer love mallory also weird obsession mass killers since mother killed massacre waco texas charles whitman cinematography superb different colours shadows styles create feeling disorientation green colour evident green resemble sickness killers drugstore looking rattlesnake antidote camera work insane shaky buzzy takes determination get use accept highly unorthodox psychedelic unusual natural born killers glamourise existence insane murderers questions public may fuel attribute although review sound quite positive dislike film quentin tarantino originally wrote script film pleased altered screenplay asked name removed see mildly interesting times natural born killers mess picture'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the characteristics of False Positives for example?\n",
    "# Good practice when doing analysis\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test[(y_test.values==0) & (valid_predictions==1)][0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.12365225e-05 6.30629823e-05 6.51341314e-05 ... 2.31419233e-04\n",
      " 2.13784608e-04 1.06538827e-04]\n"
     ]
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "# returns relative importance of all features.\n",
    "# they are in the order of the columns\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "['bad', 'worst', 'waste', 'great', 'awful', 'excellent', 'wonderful', 'worse', 'terrible', 'best']\n"
     ]
    }
   ],
   "source": [
    "# sort importance scores\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "top_10 = indices[:10]\n",
    "\n",
    "# Get top ten features\n",
    "print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# put everything together in a function\n",
    "\n",
    "def predict_sentiment(cleaned_reviews, y=train[\"sentiment\"]):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    return(forest,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Original cleaned to lemmatized and stemmed data set\n",
    "\n",
    "Now carry out the same analysis as above but on the `review_clean_ps` and `review_clean_wnl`. \n",
    "\n",
    "What data preprocessing strategy worked the best? Why do you think that is? (Feel free to change the number of features extracted in the bag of words model and the number of trees in the random forest model (i.e. the hyperparameters in our model), to see how it effects your accuracy. Is the accuracy better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.833\n",
      "\n",
      "#####\n",
      "Porter Stemmer\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8294\n",
      "\n",
      "#####\n",
      "Lemmatizing\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8266\n",
      "CPU times: user 16.8 s, sys: 255 ms, total: 17.1 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Original Reviews')\n",
    "forest1,vec1 = predict_sentiment(review_clean_original)\n",
    "print('\\n#####\\nPorter Stemmer')\n",
    "forest2,vec2 = predict_sentiment(review_clean_ps)\n",
    "print('\\n#####\\nLemmatizing')\n",
    "forest3,vec3 = predict_sentiment(review_clean_wnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It  seems like Porter Stemmer and Lemmatizing does not effect the results as much as we thought\n",
    "This is just what Sebastian Raschka points out in his book Python Machine Learning:\n",
    "\n",
    "```\n",
    "The Porter stemming algorithm is probably the oldest and simplest\n",
    "stemming algorithm. Other popular stemming algorithms include the\n",
    "newer Snowball stemmer (Porter2 or \"English\" stemmer) or the Lancaster\n",
    "stemmer (Paice-Husk stemmer), which is faster but also more aggressive\n",
    "than the Porter stemmer. Those alternative stemming algorithms are also\n",
    "available through the NLTK package (http://www.nltk.org/api/\n",
    "nltk.stem.html).\n",
    "\n",
    "While stemming can create non-real words, such as thu, (from thus) as\n",
    "shown in the previous example, a technique called lemmatization aims to\n",
    "obtain the canonical (grammatically correct) forms of individual words—\n",
    "the so-called lemmas. However, lemmatization is computationally more\n",
    "diffcult and expensive compared to stemming and, in practice, it has\n",
    "been observed that stemming and lemmatization have little impact on the\n",
    "performance of text classifcation (Michal Toman, Roman Tesar, and Karel\n",
    "Jezek. Infuence of word normalization on text classifcation. Proceedings of\n",
    "InSciT, pages 354–358, 2006).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for the different corporas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vectorizer,forest in zip([vec1, vec2, vec3],[forest1,forest2,forest3]):\n",
    "    print('TOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "sentences = [review.split() for review in review_clean_original]\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also use pretrained word2vec models that:\n",
    "#Download the Google pretrained model from,it’s 1.5GB :\n",
    "#https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "#Once you donload save unzip the file and you should will get another zip file named\n",
    "#GoogleNews-vectors-negative300.bin. \n",
    "\n",
    "\n",
    "# Gmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary count of the model\n",
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vocabulary words\n",
    "model['stuff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vocabulary words\n",
    "vocab_tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity of words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model.similarity('movie','film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('actor','actress')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.similarity('boring','dull')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['boy','woman'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"man woman ok kill\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"france man germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load the trained modelNumeric Representations of Words\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained earlier consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"wv.syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary count of the model\n",
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))\n",
    "\n",
    "# Get distributional representation of each word\n",
    "X = model[vocab_tmp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model[['hi','wow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "# get two principle components of the feature space\n",
    "pca= decomposition.PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# set figure settings\n",
    "plt.figure(figsize=(15,15),dpi=200)\n",
    "\n",
    "# save pca values and vocab in dataframe df\n",
    "df = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\n",
    "df.columns = ['x', 'y', 'word']\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Ist principal component\")\n",
    "plt.ylabel('2nd principal component')\n",
    "\n",
    "\n",
    "plt.scatter(x=pca[:, 0], y=pca[:, 1],s=3)\n",
    "for i, word in enumerate(df['word'][0:100]):\n",
    "    plt.annotate(word, (df['x'].iloc[i], df['y'].iloc[i]),c='red')\n",
    "plt.title(\"PCA Embedding\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "## A popular non-linear dimensionality reduction technique that preserves greatly thge local \n",
    "## and global structure of the data. Essentially tries to reconstruct the subspace in which the \n",
    "## data exists\n",
    "# '''This will take time to run'''\n",
    "# \n",
    "# from sklearn import manifold\n",
    "# tsne = manifold.TSNE(n_components=2)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# \n",
    "# # set figure settings\n",
    "# plt.figure(figsize=(10,10),dpi=100)\n",
    "# \n",
    "# # save pca values and vocab in dataframe df\n",
    "# df2 = pd.concat([pd.DataFrame(pca),pd.Series(vocab_tmp)],axis=1)\n",
    "# df2.columns = ['x', 'y', 'word']\n",
    "# \n",
    "# \n",
    "# plt.scatter(df2['x'][0:500], df2['y'][0:500],s=3)\n",
    "# for i, word in enumerate(df2['word'][0:500]):\n",
    "#     plt.annotate(word, (df2['x'].iloc[i], df2['y'].iloc[i]))\n",
    "# plt.title(\"Tsne Embedding\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Use Word Vectors to create a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Averaging to get feature encoding of review:\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method is to simply average the word vectors in a given review (for this purpose, remov stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from earlier sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(review, model):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    featureVec =[]\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for n,word in enumerate(review):\n",
    "        if word in index2word_set: \n",
    "            featureVec.append(model[word])\n",
    "            \n",
    "    # Average the word vectors for a \n",
    "    featureVec = np.mean(featureVec,axis=0)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one \n",
    "    \n",
    "    reviewFeatureVecs = []\n",
    "    # Loop through the reviews\n",
    "    for counter,review in enumerate(reviews):\n",
    "        \n",
    "        # Print a status message every 5000th review\n",
    "        if counter%5000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        vector= makeFeatureVec(review, model)\n",
    "        reviewFeatureVecs.append(vector)\n",
    "            \n",
    "    return reviewFeatureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# # CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_sentiment(cleaned_reviews, y=train[\"sentiment\"],max_features=1200):\n",
    "    '''This function will:\n",
    "    1. Convert reviews into feature vectors using word2vec.\n",
    "    2. split data into train and test set.\n",
    "    3. train a random forest model using train n-gram counts and y (labels)\n",
    "    4. test the model on your test split\n",
    "    5. print accuracy of sentiment prediction on test and training data\n",
    "    6. print confusion matrix on test data results\n",
    "\n",
    "    To change n-gram type, set value of ngram argument\n",
    "    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n",
    "\n",
    "    print(\"1.Creating Feature vectors using word2vec...\\n\")\n",
    "\n",
    "    trainDataVecs = getAvgFeatureVecs( cleaned_reviews, model)\n",
    "    \n",
    "   \n",
    "    print(\"\\n2.Splitting dataset into train and test sets...\\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    trainDataVecs, y, random_state=0, test_size=.2)\n",
    "\n",
    "   \n",
    "    print(\"3. Training the random forest classifier...\\n\")\n",
    "    \n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100) \n",
    "    \n",
    "    # Fit the forest to the training set, word2vecfeatures \n",
    "    # and the sentiment labels as the target variable\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(X_train)\n",
    "    test_predictions = forest.predict(X_test)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"=================Training Statistics======================\\n\")\n",
    "    print(\"The training accuracy is: \", train_acc)\n",
    "    print(\"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sentiment(cleaned_reviews=review_clean_original, y=train[\"sentiment\"],max_features=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec5'></div>\n",
    "\n",
    "## Appendix Kaggle submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to submit to Kaggle\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_cleaner( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"BoW_results.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to improve accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (worth 1p)\n",
    "\n",
    "* **Question:** Preprocess the reviews in any way you find suitable and build your own ML model that can predict the sentiment of movie reviews. Credit will be given if you can obtain a prediction accuracy of over 87%, when predicting the sentiments on a random (your model will be retrained on a new training set and a random validation set will be picked out. Train your model on 20000 reviews (with their sentiment as the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
