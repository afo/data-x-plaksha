{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcdSwgid4IED"
   },
   "source": [
    "Source: Huggingface.co\n",
    "\n",
    "# Quick tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAgsjqE94IED"
   },
   "source": [
    "Get up and running with ðŸ¤— Transformers! Start using the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) for rapid inference, and quickly load a pretrained model and tokenizer with an [AutoClass](https://huggingface.co/docs/transformers/master/en/./model_doc/auto) to solve your text, vision or audio task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwiy3qYG4IEG"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN3qpChg4IEH"
   },
   "source": [
    "[pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) is the easiest way to use a pretrained model for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCqaWAXj4IEI"
   },
   "source": [
    "The [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) supports many common tasks out-of-the-box:\n",
    "\n",
    "**Text**:\n",
    "* Sentiment analysis: classify the polarity of a given text.\n",
    "* Text generation (in English): generate text from a given input.\n",
    "* Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.).\n",
    "* Question answering: extract the answer from the context, given some context and a question.\n",
    "* Fill-mask: fill in the blank given a text with masked words.\n",
    "* Summarization: generate a summary of a long sequence of text or document.\n",
    "* Translation: translate text into another language.\n",
    "* Feature extraction: create a tensor representation of the text.\n",
    "\n",
    "**Image**:\n",
    "* Image classification: classify an image.\n",
    "* Image segmentation: classify every pixel in an image.\n",
    "* Object detection: detect objects within an image.\n",
    "\n",
    "**Audio**:\n",
    "* Audio classification: assign a label to a given segment of audio.\n",
    "* Automatic speech recognition (ASR): transcribe audio data into text.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "For more details about the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) and associated tasks, refer to the documentation [here](https://huggingface.co/docs/transformers/master/en/./main_classes/pipelines).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxY1T4-R4IEI"
   },
   "source": [
    "### Pipeline usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRNZ-Sg34IEI"
   },
   "source": [
    "In the following example, you will use the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) for sentiment analysis.\n",
    "\n",
    "Import [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) and specify the task you want to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that you've installed huggingface locally\n",
    "#!conda install -c huggingface transformers\n",
    "#!conda install -c conda-forge huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yZG7YNRh4IEJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL9kxZzj4IEJ"
   },
   "source": [
    "The pipeline downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hPFLsotJ4IEJ",
    "outputId": "9156be76-e68b-42ed-d665-cdc86db0168d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmXy9WVd4IEK"
   },
   "source": [
    "For more than one sentence, pass a list of sentences to the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) which returns a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \\\n",
    "                      \"We hope you do not hate it.\", \\\n",
    "                      \"Huggingface is the worst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6h9gvl7c4IEK",
    "outputId": "5b0166e2-1b00-4738-db19-acc81069bc8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: POSITIVE, with score: 0.97\n",
      "label: NEGATIVE, with score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pKdOz4z4IEK"
   },
   "source": [
    "The [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) can also iterate over an entire dataset. Start by installing the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/) library:\n",
    "\n",
    "```bash\n",
    "pip install datasets \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jmIcrxr4IEL"
   },
   "source": [
    "### Use another model and tokenizer in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ_PnzlL4IEL"
   },
   "source": [
    "The [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) can accommodate any model from the [Model Hub](https://huggingface.co/models), making it easy to adapt the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline) for other use-cases. For example, if you'd like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) fine-tuned for sentiment analysis. Great, let's use this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BxxA6t734IEM"
   },
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGPUM9hs4IEM"
   },
   "source": [
    "Use the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and ['AutoTokenizer'] to load the pretrained model and it's associated tokenizer (more on an `AutoClass` below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aXWfA8mp4IEM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name) #second step, classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #first step, tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHPpjrvg4IEM"
   },
   "source": [
    "Then you can specify the model and tokenizer in the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline), and apply the `classifier` on your target text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "THroNovg4IEM",
    "outputId": "5382f06a-73a0-4e9a-a34f-bd3377e4fc57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '4 stars', 'score': 0.38422518968582153}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"das ist gut ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.4348497688770294}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('Cest pas bon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.6065455079078674}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('muy bien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8LzZ6VU4IEN"
   },
   "source": [
    "If you can't find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our [fine-tuning tutorial](https://huggingface.co/docs/transformers/master/en/./training) to learn how. Finally, after you've fine-tuned your pretrained model, please consider sharing it (see tutorial [here](https://huggingface.co/docs/transformers/master/en/./model_sharing)) with the community on the Model Hub to democratize NLP for everyone! ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# End here\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkdkMUI34IEN"
   },
   "source": [
    "## AutoClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsyzBZE94IEN"
   },
   "source": [
    "Under the hood, the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer) classes work together to power the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline). An [AutoClass](https://huggingface.co/docs/transformers/master/en/./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate `AutoClass` for your task and it's associated tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer). \n",
    "\n",
    "Let's return to our example and see how you can use the `AutoClass` to replicate the results of the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfk24yPH4IEN"
   },
   "source": [
    "### AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2_WWN_E4IEO"
   },
   "source": [
    "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called *tokens*. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization [here](https://huggingface.co/docs/transformers/master/en/./tokenizer_summary)). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
    "\n",
    "Load a tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkm9SncB4IEO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AfQgVLW4IEO"
   },
   "source": [
    "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's *vocabulary*.\n",
    "\n",
    "Pass your text to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg21xziW4IEO",
    "outputId": "a0815119-5664-4258-e293-b65778c1ce6f"
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC1MKzvu4IEO"
   },
   "source": [
    "The tokenizer will return a dictionary containing:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/master/en/./glossary#input-ids): numerical representions of your tokens.\n",
    "* [atttention_mask](https://huggingface.co/docs/transformers/master/en/.glossary#attention-mask): indicates which tokens should be attended to.\n",
    "\n",
    "Just like the [pipeline()](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhGCydgJ4IEO"
   },
   "source": [
    "Read the [preprocessing](https://huggingface.co/docs/transformers/master/en/./preprocessing) tutorial for more details about tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MymdICqz4IEO"
   },
   "source": [
    "### AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy5XOAix4IEO"
   },
   "source": [
    "ðŸ¤— Transformers provides a simple and unified way to load pretrained instances. This means you can load an [AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) for the task. Since you are doing text - or sequence - classification, load [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification). The TensorFlow equivalent is simply [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whmo6NYi4IEP"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "See the [task summary](https://huggingface.co/docs/transformers/master/en/./task_summary) for which [AutoModel](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModel) class to use for which task.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding `**`. For TensorFlow models, pass the dictionary keys directly to the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQP6yX544IEP"
   },
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgJw7hVs4IEP"
   },
   "source": [
    "The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtvwgdFt4IEP",
    "outputId": "a905e3f5-9386-4b85-8a5b-0f1354f15fb0"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKz5Am_z4IEQ"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "All ðŸ¤— Transformers models (PyTorch or TensorFlow) outputs the tensors *before* the final activation\n",
    "function (like softmax) because the final activation function is often fused with the loss.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) so you can use them in your usual training loop. However, to make things easier, ðŸ¤— Transformers provides a [Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer) class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the `fit` method from [Keras](https://keras.io/). Refer to the [training tutorial](https://huggingface.co/docs/transformers/master/en/./training) for more details.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ðŸ¤— Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.\n",
    "The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `None` are ignored.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz53fMmf4IEQ"
   },
   "source": [
    "### Save a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1BzJfO74IEQ"
   },
   "source": [
    "Once your model is fine-tuned, you can save it with its tokenizer using [PreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8XFlNpn4IEQ"
   },
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtnLMlZH4IEQ"
   },
   "source": [
    "When you are ready to use the model again, reload it with [PreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_c6rJJq4IEQ"
   },
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQCkHC0z4IEQ"
   },
   "source": [
    "One particularly cool ðŸ¤— Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf` parameter can convert the model from one framework to the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGuM8CT64IEQ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of quicktour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
