{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "## Data-X: Titanic Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Several public Kaggle Kernels, edits by Alexander Fred Ojala & Kevin Li\n",
    "\n",
    "<img src=\"data/Titanic_Variable.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "Install xgboost package in  your pyhton  enviroment:\n",
    "\n",
    "try:\n",
    "```\n",
    "$ conda install py-xgboost\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# You can also install the package by running the line below\n",
    "# directly in your notebook\n",
    "''';\n",
    "\n",
    "#!conda install py-xgboost --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "pd.set_option('display.max_columns', 100) # Print 100 Pandas columns\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bays\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier #stochastic gradient descent\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Plot styling\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "plt.rcParams[ 'figure.figsize' ] = 10 , 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fancy plot to look at distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special distribution plot (will be used later)\n",
    "def plot_distribution( df , var , target , **kwargs ):\n",
    "    row = kwargs.get( 'row' , None )\n",
    "    col = kwargs.get( 'col' , None )\n",
    "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n",
    "    facet.map( sns.kdeplot , var , shade= True )\n",
    "    facet.set( xlim=( 0 , df[ var ].max() ) )\n",
    "    facet.add_legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References to material we won't cover in detail:\n",
    "\n",
    "* **Gradient Boosting:** http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "* **Naive Bayes:** http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "* **Perceptron:** http://aass.oru.se/~lilien/ml/seminars/2007_02_01b-Janecek-Perceptron.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "# NOTE! When we change train_df or test_df the objects in combine \n",
    "# will also change\n",
    "# (combine is only a pointer to the objects)\n",
    "\n",
    "\n",
    "# combine is used to ensure whatever preprocessing is done\n",
    "# on training data is also done on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Anlysis (EDA)\n",
    "We will analyze the data to see how we can work with it and what makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame information (null, data type etc)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(figsize=(13,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set?\n",
    "train_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[0]/(sum(_)) #base line for prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.tools.plotting.scatter_matrix(train_df,figsize=(13,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the Data\n",
    "\n",
    "> `PassengerId` is a random number (incrementing index) and thus does not contain any valuable information. \n",
    ">\n",
    ">`Survived, Passenger Class, Age Siblings Spouses, Parents Children` and `Fare` are numerical values -- so we don't need to transform them, but we might want to group them (i.e. create categorical variables). \n",
    ">\n",
    ">`Sex, Embarked` are categorical features that we need to map to integer values. `Name, Ticket` and `Cabin` might also contain valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of the train and test datasets\n",
    "print(\"Shapes Before: (train) (test) = \", \\\n",
    "      train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Ticket', 'Cabin', need to do it for both test\n",
    "# and training\n",
    "\n",
    "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "print(\"Shapes After: (train) (test) =\", train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are null values in the datasets\n",
    "\n",
    "print(train_df.isnull().sum())\n",
    "print()\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "## 1: The Title of the person is a feature that can predict survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List example titles in Name column\n",
    "train_df.Name[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the Name column we will extract title of each passenger\n",
    "# and save that in a column in the dataset called 'Title'\n",
    "# if you want to match Titles or names with any other expression\n",
    "# refer to this tutorial on regex in python:\n",
    "# https://www.tutorialspoint.com/python/python_reg_expressions.htm\n",
    "\n",
    "# Create new column called title\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.',\\\n",
    "                                                expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that our titles makes sense (by comparing to sex)\n",
    "\n",
    "pd.crosstab(train_df['Title'], train_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for test set\n",
    "pd.crosstab(test_df['Title'], test_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see common titles like Miss, Mrs, Mr, Master are dominant, we will\n",
    "# correct some Titles to standard forms and replace the rarest titles \n",
    "# with single name 'Rare'\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].\\\n",
    "                  replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr',\\\n",
    "                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') #Mademoiselle\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') #Madame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have more logical titles, and a few groups\n",
    "# we can plot the survival chance for each title\n",
    "\n",
    "train_df[['Title', 'Survived']].groupby(['Title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also plot it\n",
    "sns.countplot(x='Survived', hue=\"Title\", data=train_df, order=[1,0])\n",
    "plt.xticks(range(2),['Made it','Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title dummy mapping\n",
    "# Map titles to binary dummy columns\n",
    "for dataset in combine:\n",
    "    binary_encoded = pd.get_dummies(dataset.Title)\n",
    "    newcols = binary_encoded.columns\n",
    "    dataset[newcols] = binary_encoded\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['PassengerId','Name', 'Title'], axis=1)\n",
    "test_df = test_df.drop(['Name', 'Title'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Sex column to binary (male = 0, female = 1) categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex']. \\\n",
    "        map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values for age\n",
    "We will now guess values of age based on sex (male / female) \n",
    "and socioeconomic class (1st,2nd,3rd) of the passenger.\n",
    "\n",
    "The row indicates the sex, male = 0, female = 1\n",
    "\n",
    "More refined estimate than only taking the median / mean etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_ages = np.zeros((2,3),dtype=int) #initialize\n",
    "guess_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA's for the Age columns\n",
    "# with \"qualified guesses\"\n",
    "\n",
    "for idx,dataset in enumerate(combine):\n",
    "    if idx==0:\n",
    "        print('Working on Training Data set\\n')\n",
    "    else:\n",
    "        print('-'*35)\n",
    "        print('Working on Test Data set\\n')\n",
    "    \n",
    "    print('Guess values of age based on sex and pclass of the passenger...')\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0,3):\n",
    "            guess_df = dataset[(dataset['Sex'] == i) \\\n",
    "                        &(dataset['Pclass'] == j+1)]['Age'].dropna()\n",
    "\n",
    "            # Extract the median age for this group\n",
    "            # (less sensitive) to outliers\n",
    "            age_guess = guess_df.median()\n",
    "          \n",
    "            # Convert random age float to int\n",
    "            guess_ages[i,j] = int(age_guess)\n",
    "    \n",
    "            \n",
    "    print('Guess_Age table:\\n',guess_ages)\n",
    "    print ('\\nAssigning age values to NAN age values in the dataset...')\n",
    "    \n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) \\\n",
    "                    & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n",
    "                    \n",
    "\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    print()\n",
    "print('Done!')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split age into bands / categorical ranges and look at survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bands\n",
    "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n",
    "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False)\\\n",
    "                    .mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived \n",
    "# or did not survive\n",
    "\n",
    "plot_distribution( train_df , var = 'Age' , target = 'Survived' ,\\\n",
    "                  row = 'Sex' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Age column to\n",
    "# map Age ranges (AgeBands) to integer values of categorical type \n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']=4\n",
    "train_df.head()\n",
    "\n",
    "# Note we could just run \n",
    "# dataset['Age'] = pd.cut(dataset['Age'], 5,labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove AgeBand from before\n",
    "train_df = train_df.drop(['AgeBand'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create variable for Family Size\n",
    "\n",
    "How did the number of people the person traveled with impact the chance of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp = Number of Sibling / Spouses\n",
    "# Parch = Parents / Children\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    \n",
    "# Survival chance against FamilySize\n",
    "\n",
    "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "sns.countplot(x='Survived', hue=\"FamilySize\", data=train_df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable if the person was alone or not\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the binary IsAlone feature for further analysis\n",
    "\n",
    "for df in combine:\n",
    "    df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new features based on intuitive combinations\n",
    "# Here is an example when we say that the age times socioclass is a determinant factor\n",
    "for dataset in combine:\n",
    "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n",
    "\n",
    "train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Age*Class', 'Survived']].groupby(['Age*Class'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port the person embarked from\n",
    "Let's see how that influences chance of survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To replace Nan value in 'Embarked', we will use the mode\n",
    "# in 'Embaraked'. This will give us the most frequent port \n",
    "# the passengers embarked from\n",
    "\n",
    "freq_port = train_df['Embarked'].dropna().mode()[0]\n",
    "print('Most frequent port of Embarkation:',freq_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN 'Embarked' Values in the datasets\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
    "    \n",
    "train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Embarked\", data=train_df, order=[1,0])\n",
    "plt.xticks(range(2),['Made it!', 'Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical dummy variables for Embarked values\n",
    "for dataset in combine:\n",
    "    binary_encoded = pd.get_dummies(dataset.Embarked)\n",
    "    newcols = binary_encoded.columns\n",
    "    dataset[newcols] = binary_encoded\n",
    "\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Embarked\n",
    "for dataset in combine:\n",
    "    dataset.drop('Embarked', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle continuous values in the Fare column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA values in the Fares column with the median\n",
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q cut will find ranges equal to the quartile of the data\n",
    "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n",
    "train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Fare']=pd.qcut(train_df['Fare'],4,labels=np.arange(4))\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "train_df[['Fare','FareBand']].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop FareBand\n",
    "train_df = train_df.drop(['FareBand'], axis=1) \n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(7)\n",
    "# All features are approximately on the same scale\n",
    "# no need for feature engineering / normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between features \n",
    "# (uncorrelated features are generally more powerful predictors)\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train_df.corr().round(2)\\\n",
    "            ,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \\\n",
    "            linecolor='white', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Up: Machine Learning!\n",
    "Now we will Model, Predict, and Choose algorithm for conducting the classification\n",
    "Try using different classifiers to model and predict. Choose the best model from:\n",
    "* Logistic Regression\n",
    "* KNN \n",
    "* SVM\n",
    "* Naive Bayes\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Perceptron\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(\"Survived\", axis=1) # Training & Validation data\n",
    "Y = train_df[\"Survived\"] # Response / Target Variable\n",
    "\n",
    "# Since we don't have labels for the test data\n",
    "# this won't be used. It's only for Kaggle Submissions\n",
    "X_submission  = test_df.drop(\"PassengerId\", axis=1).copy() \n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set so that we test on 20% of the data\n",
    "# Note that our algorithms will never have seen the validation \n",
    "# data during training. This is to evaluate how good our estimators are.\n",
    "\n",
    "np.random.seed(1337) # set random seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn general ML workflow\n",
    "1. Instantiate model object\n",
    "2. Fit model to training data\n",
    "3. Let the model predict output for unseen data\n",
    "4. Compare predicitons with actual output to form accuracy measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression() # instantiate\n",
    "logreg.fit(X_train, Y_train) # fit\n",
    "Y_pred = logreg.predict(X_val) # predict\n",
    "acc_log = sum(Y_pred == Y_val)/len(Y_val)*100\n",
    "print('Logistic Regression accuracy:', str(round(acc_log,2)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also use scikit learn's method score\n",
    "# that predicts and then compares to validation set labels\n",
    "acc_log = logreg.score(X_val, Y_val) # evaluate\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines Classifier (non-linear kernel)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "acc_svc = svc.score(X_val, Y_val)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "acc_knn = knn.score(X_val, Y_val)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "acc_perceptron = perceptron.score(X_val, Y_val)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(n_estimators=1000)\n",
    "gradboost.fit(X_train, Y_train)\n",
    "acc_perceptron = gradboost.score(X_val, Y_val)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=1000)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "acc_random_forest = random_forest.score(X_val, Y_val)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance scores in the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at importnace of features for random forest\n",
    "\n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print ('Training accuracy Random Forest:',model.score( X , y ))\n",
    "\n",
    "plot_model_var_imp(random_forest, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compete on Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to create a Kaggle submission with a Random Forest Classifier\n",
    "Y_submission = random_forest.predict(X_submission)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_submission\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy code (not used anymore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Map title string values to numbers so that we can make predictions\n",
    "\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0) \n",
    "    # Handle missing values\n",
    "\n",
    "train_df.head()\n",
    "```\n",
    "\n",
    "```python\n",
    "# Drop the unnecessary Name column (we have the titles now)\n",
    "\n",
    "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.shape, test_df.shape\n",
    "```\n",
    "\n",
    "```python\n",
    "# Create categorical dummy variables for Embarked values\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "\n",
    "train_df.head()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
